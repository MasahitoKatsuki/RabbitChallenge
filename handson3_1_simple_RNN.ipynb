{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "3_1_simple_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MasahitoKatsuki/RabbitChallenge/blob/main/handson3_1_simple_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cNl2QA_Rnv5"
      },
      "source": [
        "# 準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkwjN1jNVAYy"
      },
      "source": [
        "## Googleドライブのマウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvFXpiH3EVC1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a016f96-95b9-4746-be0e-9b87431826e6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ub7RYdeY6pK"
      },
      "source": [
        "## sys.pathの設定"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oql7L19rEsWi"
      },
      "source": [
        "以下では，Googleドライブのマイドライブ直下にDNN_codeフォルダを置くことを仮定しています．必要に応じて，パスを変更してください．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ic2JzkvFX59"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/DNN_code')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feXB1SiLP4OL"
      },
      "source": [
        "# simple RNN\n",
        "### バイナリ加算"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "tzSWNYwxP4OM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e286ecae-b0bf-4fb4-93f5-3cdb4bf37c84"
      },
      "source": [
        "import numpy as np\n",
        "from common import functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# def d_tanh(x):\n",
        "\n",
        "\n",
        "\n",
        "# データを用意\n",
        "# 2進数の桁数\n",
        "binary_dim = 8\n",
        "# 最大値 + 1\n",
        "largest_number = pow(2, binary_dim)\n",
        "# largest_numberまで2進数を用意\n",
        "binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
        "\n",
        "input_layer_size = 2\n",
        "hidden_layer_size = 16\n",
        "output_layer_size = 1\n",
        "\n",
        "weight_init_std = 1\n",
        "learning_rate = 0.1\n",
        "\n",
        "iters_num = 10000\n",
        "plot_interval = 100\n",
        "\n",
        "# ウェイト初期化 (バイアスは簡単のため省略)\n",
        "W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n",
        "W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n",
        "W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n",
        "\n",
        "# Xavier\n",
        "\n",
        "\n",
        "# He\n",
        "\n",
        "\n",
        "\n",
        "# 勾配\n",
        "W_in_grad = np.zeros_like(W_in)\n",
        "W_out_grad = np.zeros_like(W_out)\n",
        "W_grad = np.zeros_like(W)\n",
        "\n",
        "u = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "z = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "y = np.zeros((output_layer_size, binary_dim))\n",
        "\n",
        "delta_out = np.zeros((output_layer_size, binary_dim))\n",
        "delta = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "\n",
        "all_losses = []\n",
        "\n",
        "for i in range(iters_num):\n",
        "    \n",
        "    # A, B初期化 (a + b = d)\n",
        "    a_int = np.random.randint(largest_number/2)\n",
        "    a_bin = binary[a_int] # binary encoding\n",
        "    b_int = np.random.randint(largest_number/2)\n",
        "    b_bin = binary[b_int] # binary encoding\n",
        "    \n",
        "    # 正解データ\n",
        "    d_int = a_int + b_int\n",
        "    d_bin = binary[d_int]\n",
        "    \n",
        "    # 出力バイナリ\n",
        "    out_bin = np.zeros_like(d_bin)\n",
        "    \n",
        "    # 時系列全体の誤差\n",
        "    all_loss = 0    \n",
        "    \n",
        "    # 時系列ループ\n",
        "    for t in range(binary_dim):\n",
        "        # 入力値\n",
        "        X = np.array([a_bin[ - t - 1], b_bin[ - t - 1]]).reshape(1, -1)\n",
        "        # 時刻tにおける正解データ\n",
        "        dd = np.array([d_bin[binary_dim - t - 1]])\n",
        "        \n",
        "        u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n",
        "        z[:,t+1] = functions.sigmoid(u[:,t+1])\n",
        "\n",
        "        y[:,t] = functions.sigmoid(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
        "\n",
        "\n",
        "        #誤差\n",
        "        loss = functions.mean_squared_error(dd, y[:,t])\n",
        "        \n",
        "        delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * functions.d_sigmoid(y[:,t])        \n",
        "        \n",
        "        all_loss += loss\n",
        "\n",
        "        out_bin[binary_dim - t - 1] = np.round(y[:,t])\n",
        "    \n",
        "    \n",
        "    for t in range(binary_dim)[::-1]:\n",
        "        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n",
        "\n",
        "        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_sigmoid(u[:,t+1])\n",
        "\n",
        "        # 勾配更新\n",
        "        W_out_grad += np.dot(z[:,t+1].reshape(-1,1), delta_out[:,t].reshape(-1,1))\n",
        "        W_grad += np.dot(z[:,t].reshape(-1,1), delta[:,t].reshape(1,-1))\n",
        "        W_in_grad += np.dot(X.T, delta[:,t].reshape(1,-1))\n",
        "    \n",
        "    # 勾配適用\n",
        "    W_in -= learning_rate * W_in_grad\n",
        "    W_out -= learning_rate * W_out_grad\n",
        "    W -= learning_rate * W_grad\n",
        "    \n",
        "    W_in_grad *= 0\n",
        "    W_out_grad *= 0\n",
        "    W_grad *= 0\n",
        "    \n",
        "\n",
        "    if(i % plot_interval == 0):\n",
        "        all_losses.append(all_loss)        \n",
        "        print(\"iters:\" + str(i))\n",
        "        print(\"Loss:\" + str(all_loss))\n",
        "        print(\"Pred:\" + str(out_bin))\n",
        "        print(\"True:\" + str(d_bin))\n",
        "        out_int = 0\n",
        "        for index,x in enumerate(reversed(out_bin)):\n",
        "            out_int += x * pow(2, index)\n",
        "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out_int))\n",
        "        print(\"------------\")\n",
        "\n",
        "lists = range(0, iters_num, plot_interval)\n",
        "plt.plot(lists, all_losses, label=\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iters:0\n",
            "Loss:1.711100994700102\n",
            "Pred:[0 0 0 0 0 0 0 1]\n",
            "True:[1 0 1 1 0 1 1 1]\n",
            "110 + 73 = 1\n",
            "------------\n",
            "iters:100\n",
            "Loss:1.0276969171641575\n",
            "Pred:[0 0 0 0 1 1 0 0]\n",
            "True:[0 1 0 0 1 0 1 1]\n",
            "37 + 38 = 12\n",
            "------------\n",
            "iters:200\n",
            "Loss:1.119677865080079\n",
            "Pred:[0 0 0 0 0 1 1 1]\n",
            "True:[0 1 1 1 0 0 1 0]\n",
            "47 + 67 = 7\n",
            "------------\n",
            "iters:300\n",
            "Loss:0.9166768611257987\n",
            "Pred:[0 0 0 1 0 0 0 0]\n",
            "True:[0 0 1 0 0 1 1 1]\n",
            "8 + 31 = 16\n",
            "------------\n",
            "iters:400\n",
            "Loss:0.8417817098851526\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 1 0 1 1 1]\n",
            "75 + 44 = 255\n",
            "------------\n",
            "iters:500\n",
            "Loss:0.8754179158092896\n",
            "Pred:[1 1 0 1 1 1 0 1]\n",
            "True:[0 1 1 1 1 1 0 1]\n",
            "91 + 34 = 221\n",
            "------------\n",
            "iters:600\n",
            "Loss:0.8695349428891077\n",
            "Pred:[1 1 1 1 0 1 1 1]\n",
            "True:[1 0 1 1 1 1 1 1]\n",
            "86 + 105 = 247\n",
            "------------\n",
            "iters:700\n",
            "Loss:0.9271650582201854\n",
            "Pred:[1 0 0 0 0 1 1 0]\n",
            "True:[1 1 0 1 0 0 1 0]\n",
            "103 + 107 = 134\n",
            "------------\n",
            "iters:800\n",
            "Loss:1.0369139583048517\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 0 1 1 0 1]\n",
            "8 + 37 = 0\n",
            "------------\n",
            "iters:900\n",
            "Loss:1.0093217788908797\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[1 1 0 0 0 1 1 1]\n",
            "79 + 120 = 255\n",
            "------------\n",
            "iters:1000\n",
            "Loss:1.0856802832985237\n",
            "Pred:[0 0 1 1 1 1 0 0]\n",
            "True:[0 1 1 0 0 0 0 1]\n",
            "83 + 14 = 60\n",
            "------------\n",
            "iters:1100\n",
            "Loss:0.8444737751564049\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 0 1 0 0 1]\n",
            "36 + 5 = 0\n",
            "------------\n",
            "iters:1200\n",
            "Loss:1.1126403723956104\n",
            "Pred:[1 0 1 1 0 0 0 1]\n",
            "True:[1 1 0 0 1 0 1 0]\n",
            "126 + 76 = 177\n",
            "------------\n",
            "iters:1300\n",
            "Loss:1.0250067925930264\n",
            "Pred:[0 0 0 0 1 1 0 0]\n",
            "True:[0 1 0 0 1 0 0 0]\n",
            "6 + 66 = 12\n",
            "------------\n",
            "iters:1400\n",
            "Loss:1.2576487851379314\n",
            "Pred:[0 1 1 1 0 1 0 0]\n",
            "True:[1 0 0 0 1 0 0 1]\n",
            "110 + 27 = 116\n",
            "------------\n",
            "iters:1500\n",
            "Loss:0.8493260664014779\n",
            "Pred:[1 1 1 0 0 0 1 0]\n",
            "True:[0 1 1 0 0 0 1 0]\n",
            "25 + 73 = 226\n",
            "------------\n",
            "iters:1600\n",
            "Loss:0.8178751338161827\n",
            "Pred:[0 1 1 0 1 1 0 1]\n",
            "True:[0 1 0 0 1 1 0 0]\n",
            "54 + 22 = 109\n",
            "------------\n",
            "iters:1700\n",
            "Loss:1.038654923419584\n",
            "Pred:[1 1 1 1 1 1 0 1]\n",
            "True:[0 1 1 1 0 0 0 1]\n",
            "67 + 46 = 253\n",
            "------------\n",
            "iters:1800\n",
            "Loss:0.8373898626060051\n",
            "Pred:[1 1 1 1 0 0 0 0]\n",
            "True:[1 1 1 0 0 1 0 0]\n",
            "110 + 118 = 240\n",
            "------------\n",
            "iters:1900\n",
            "Loss:0.8986753611817377\n",
            "Pred:[1 1 1 0 0 0 0 1]\n",
            "True:[1 0 0 0 1 0 0 0]\n",
            "20 + 116 = 225\n",
            "------------\n",
            "iters:2000\n",
            "Loss:0.8222128170290689\n",
            "Pred:[1 0 1 0 0 0 0 0]\n",
            "True:[1 0 1 0 1 0 0 1]\n",
            "80 + 89 = 160\n",
            "------------\n",
            "iters:2100\n",
            "Loss:0.9527384911109578\n",
            "Pred:[0 1 1 0 1 1 0 0]\n",
            "True:[0 1 0 0 0 1 0 1]\n",
            "30 + 39 = 108\n",
            "------------\n",
            "iters:2200\n",
            "Loss:0.9345606066816781\n",
            "Pred:[0 1 1 0 0 1 0 1]\n",
            "True:[0 1 0 0 1 1 1 0]\n",
            "26 + 52 = 101\n",
            "------------\n",
            "iters:2300\n",
            "Loss:0.8249369132269789\n",
            "Pred:[1 1 0 0 1 0 1 0]\n",
            "True:[0 1 1 1 1 0 1 0]\n",
            "49 + 73 = 202\n",
            "------------\n",
            "iters:2400\n",
            "Loss:0.79247831748565\n",
            "Pred:[1 1 1 1 1 1 0 0]\n",
            "True:[1 1 0 1 1 1 1 0]\n",
            "104 + 118 = 252\n",
            "------------\n",
            "iters:2500\n",
            "Loss:0.8765001988362952\n",
            "Pred:[0 1 1 0 1 0 0 0]\n",
            "True:[0 1 0 0 0 0 0 1]\n",
            "20 + 45 = 104\n",
            "------------\n",
            "iters:2600\n",
            "Loss:0.9190749051393059\n",
            "Pred:[1 1 1 1 0 1 1 0]\n",
            "True:[1 0 1 1 1 0 0 0]\n",
            "63 + 121 = 246\n",
            "------------\n",
            "iters:2700\n",
            "Loss:0.8204252959543362\n",
            "Pred:[1 1 1 0 1 0 0 0]\n",
            "True:[1 1 0 0 1 1 1 1]\n",
            "116 + 91 = 232\n",
            "------------\n",
            "iters:2800\n",
            "Loss:0.5433124844062993\n",
            "Pred:[0 0 1 1 0 1 1 1]\n",
            "True:[0 0 1 1 0 1 1 1]\n",
            "21 + 34 = 55\n",
            "------------\n",
            "iters:2900\n",
            "Loss:0.6209964952977268\n",
            "Pred:[0 1 1 1 1 0 0 0]\n",
            "True:[0 1 1 1 1 0 1 0]\n",
            "108 + 14 = 120\n",
            "------------\n",
            "iters:3000\n",
            "Loss:0.5757740230051864\n",
            "Pred:[1 0 0 0 1 1 0 1]\n",
            "True:[1 0 0 0 1 0 0 1]\n",
            "39 + 98 = 141\n",
            "------------\n",
            "iters:3100\n",
            "Loss:0.38074114730289815\n",
            "Pred:[0 0 0 1 0 1 0 1]\n",
            "True:[0 0 0 1 0 1 0 1]\n",
            "8 + 13 = 21\n",
            "------------\n",
            "iters:3200\n",
            "Loss:0.3491669278479217\n",
            "Pred:[0 1 1 0 0 1 0 1]\n",
            "True:[0 1 1 0 0 1 0 1]\n",
            "3 + 98 = 101\n",
            "------------\n",
            "iters:3300\n",
            "Loss:0.2769216127525885\n",
            "Pred:[0 1 0 1 0 1 0 1]\n",
            "True:[0 1 0 1 0 1 0 1]\n",
            "30 + 55 = 85\n",
            "------------\n",
            "iters:3400\n",
            "Loss:0.3170695335867411\n",
            "Pred:[1 0 1 0 1 1 1 1]\n",
            "True:[1 0 1 0 1 1 0 1]\n",
            "52 + 121 = 175\n",
            "------------\n",
            "iters:3500\n",
            "Loss:0.5591599813478502\n",
            "Pred:[1 0 0 0 0 1 1 1]\n",
            "True:[1 1 0 0 0 1 1 1]\n",
            "74 + 125 = 135\n",
            "------------\n",
            "iters:3600\n",
            "Loss:0.06658583485699897\n",
            "Pred:[1 0 1 0 1 1 1 0]\n",
            "True:[1 0 1 0 1 1 1 0]\n",
            "127 + 47 = 174\n",
            "------------\n",
            "iters:3700\n",
            "Loss:0.30557940971065417\n",
            "Pred:[1 1 0 1 0 0 1 1]\n",
            "True:[1 1 0 1 0 0 1 1]\n",
            "84 + 127 = 211\n",
            "------------\n",
            "iters:3800\n",
            "Loss:0.3072803183100405\n",
            "Pred:[1 0 1 0 0 1 1 1]\n",
            "True:[1 0 1 0 0 1 1 1]\n",
            "127 + 40 = 167\n",
            "------------\n",
            "iters:3900\n",
            "Loss:0.06884198536865091\n",
            "Pred:[0 1 0 0 1 1 0 1]\n",
            "True:[0 1 0 0 1 1 0 1]\n",
            "62 + 15 = 77\n",
            "------------\n",
            "iters:4000\n",
            "Loss:0.38620205367003185\n",
            "Pred:[0 0 1 1 1 1 0 0]\n",
            "True:[0 1 1 1 1 1 0 0]\n",
            "76 + 48 = 60\n",
            "------------\n",
            "iters:4100\n",
            "Loss:0.13319751299402363\n",
            "Pred:[0 1 1 0 1 1 1 1]\n",
            "True:[0 1 1 0 1 1 1 1]\n",
            "21 + 90 = 111\n",
            "------------\n",
            "iters:4200\n",
            "Loss:0.10987307182338042\n",
            "Pred:[1 0 1 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 0 0 0]\n",
            "98 + 62 = 160\n",
            "------------\n",
            "iters:4300\n",
            "Loss:0.11727023125477856\n",
            "Pred:[0 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 1 1 1 1 1]\n",
            "44 + 83 = 127\n",
            "------------\n",
            "iters:4400\n",
            "Loss:0.015057239552579661\n",
            "Pred:[0 1 0 1 0 1 1 0]\n",
            "True:[0 1 0 1 0 1 1 0]\n",
            "41 + 45 = 86\n",
            "------------\n",
            "iters:4500\n",
            "Loss:0.12320810583690092\n",
            "Pred:[0 1 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 0 0 0 0]\n",
            "42 + 22 = 64\n",
            "------------\n",
            "iters:4600\n",
            "Loss:0.05747033155216516\n",
            "Pred:[1 0 0 0 0 1 1 0]\n",
            "True:[1 0 0 0 0 1 1 0]\n",
            "32 + 102 = 134\n",
            "------------\n",
            "iters:4700\n",
            "Loss:0.04516042353246699\n",
            "Pred:[0 0 1 0 1 1 1 0]\n",
            "True:[0 0 1 0 1 1 1 0]\n",
            "40 + 6 = 46\n",
            "------------\n",
            "iters:4800\n",
            "Loss:0.04571902283879671\n",
            "Pred:[0 1 1 1 1 1 0 0]\n",
            "True:[0 1 1 1 1 1 0 0]\n",
            "72 + 52 = 124\n",
            "------------\n",
            "iters:4900\n",
            "Loss:0.007671622031395722\n",
            "Pred:[0 1 1 1 1 0 0 0]\n",
            "True:[0 1 1 1 1 0 0 0]\n",
            "69 + 51 = 120\n",
            "------------\n",
            "iters:5000\n",
            "Loss:0.052648014726739904\n",
            "Pred:[0 1 1 0 0 0 1 0]\n",
            "True:[0 1 1 0 0 0 1 0]\n",
            "12 + 86 = 98\n",
            "------------\n",
            "iters:5100\n",
            "Loss:0.004365573140740137\n",
            "Pred:[0 0 1 1 0 1 0 0]\n",
            "True:[0 0 1 1 0 1 0 0]\n",
            "51 + 1 = 52\n",
            "------------\n",
            "iters:5200\n",
            "Loss:0.02012349792232008\n",
            "Pred:[0 1 1 0 0 0 1 1]\n",
            "True:[0 1 1 0 0 0 1 1]\n",
            "69 + 30 = 99\n",
            "------------\n",
            "iters:5300\n",
            "Loss:0.026509287993160886\n",
            "Pred:[0 1 1 1 0 1 1 1]\n",
            "True:[0 1 1 1 0 1 1 1]\n",
            "78 + 41 = 119\n",
            "------------\n",
            "iters:5400\n",
            "Loss:0.020264633352406494\n",
            "Pred:[1 1 0 0 0 1 0 1]\n",
            "True:[1 1 0 0 0 1 0 1]\n",
            "71 + 126 = 197\n",
            "------------\n",
            "iters:5500\n",
            "Loss:0.005814175584806633\n",
            "Pred:[1 1 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 0 0 0 0]\n",
            "75 + 117 = 192\n",
            "------------\n",
            "iters:5600\n",
            "Loss:0.021574607978139915\n",
            "Pred:[0 1 0 1 1 1 1 1]\n",
            "True:[0 1 0 1 1 1 1 1]\n",
            "28 + 67 = 95\n",
            "------------\n",
            "iters:5700\n",
            "Loss:0.0024138096224151197\n",
            "Pred:[1 0 1 0 1 1 1 0]\n",
            "True:[1 0 1 0 1 1 1 0]\n",
            "47 + 127 = 174\n",
            "------------\n",
            "iters:5800\n",
            "Loss:0.014540393384597174\n",
            "Pred:[1 1 0 0 0 1 0 1]\n",
            "True:[1 1 0 0 0 1 0 1]\n",
            "76 + 121 = 197\n",
            "------------\n",
            "iters:5900\n",
            "Loss:0.010845765228492777\n",
            "Pred:[1 1 0 1 1 0 0 1]\n",
            "True:[1 1 0 1 1 0 0 1]\n",
            "123 + 94 = 217\n",
            "------------\n",
            "iters:6000\n",
            "Loss:0.02914419331898362\n",
            "Pred:[1 1 0 1 0 1 1 0]\n",
            "True:[1 1 0 1 0 1 1 0]\n",
            "98 + 116 = 214\n",
            "------------\n",
            "iters:6100\n",
            "Loss:0.008759958168575678\n",
            "Pred:[0 0 1 1 1 0 0 1]\n",
            "True:[0 0 1 1 1 0 0 1]\n",
            "1 + 56 = 57\n",
            "------------\n",
            "iters:6200\n",
            "Loss:0.007916547067315639\n",
            "Pred:[1 0 1 0 0 0 0 1]\n",
            "True:[1 0 1 0 0 0 0 1]\n",
            "111 + 50 = 161\n",
            "------------\n",
            "iters:6300\n",
            "Loss:0.0014965854016567154\n",
            "Pred:[1 1 0 0 0 1 0 0]\n",
            "True:[1 1 0 0 0 1 0 0]\n",
            "103 + 93 = 196\n",
            "------------\n",
            "iters:6400\n",
            "Loss:0.0015253131987584348\n",
            "Pred:[0 0 1 1 1 1 0 0]\n",
            "True:[0 0 1 1 1 1 0 0]\n",
            "43 + 17 = 60\n",
            "------------\n",
            "iters:6500\n",
            "Loss:0.008455688885837982\n",
            "Pred:[1 0 0 1 1 0 1 1]\n",
            "True:[1 0 0 1 1 0 1 1]\n",
            "91 + 64 = 155\n",
            "------------\n",
            "iters:6600\n",
            "Loss:0.022994983706617834\n",
            "Pred:[1 0 0 0 1 0 1 0]\n",
            "True:[1 0 0 0 1 0 1 0]\n",
            "92 + 46 = 138\n",
            "------------\n",
            "iters:6700\n",
            "Loss:0.006680529141540147\n",
            "Pred:[1 1 0 0 0 0 0 1]\n",
            "True:[1 1 0 0 0 0 0 1]\n",
            "113 + 80 = 193\n",
            "------------\n",
            "iters:6800\n",
            "Loss:0.008089472820625198\n",
            "Pred:[1 0 0 0 1 1 0 1]\n",
            "True:[1 0 0 0 1 1 0 1]\n",
            "55 + 86 = 141\n",
            "------------\n",
            "iters:6900\n",
            "Loss:0.01026064797165566\n",
            "Pred:[1 0 0 0 0 0 1 1]\n",
            "True:[1 0 0 0 0 0 1 1]\n",
            "26 + 105 = 131\n",
            "------------\n",
            "iters:7000\n",
            "Loss:0.006334742563595175\n",
            "Pred:[1 0 0 1 1 1 0 1]\n",
            "True:[1 0 0 1 1 1 0 1]\n",
            "101 + 56 = 157\n",
            "------------\n",
            "iters:7100\n",
            "Loss:0.007389112012707\n",
            "Pred:[0 0 1 0 0 1 1 1]\n",
            "True:[0 0 1 0 0 1 1 1]\n",
            "31 + 8 = 39\n",
            "------------\n",
            "iters:7200\n",
            "Loss:0.01548904504876339\n",
            "Pred:[0 0 1 0 1 1 1 0]\n",
            "True:[0 0 1 0 1 1 1 0]\n",
            "6 + 40 = 46\n",
            "------------\n",
            "iters:7300\n",
            "Loss:0.0009714950578512169\n",
            "Pred:[1 1 0 0 1 1 1 0]\n",
            "True:[1 1 0 0 1 1 1 0]\n",
            "113 + 93 = 206\n",
            "------------\n",
            "iters:7400\n",
            "Loss:0.005326818769791322\n",
            "Pred:[1 1 0 0 0 1 1 1]\n",
            "True:[1 1 0 0 0 1 1 1]\n",
            "89 + 110 = 199\n",
            "------------\n",
            "iters:7500\n",
            "Loss:0.014330413150331227\n",
            "Pred:[1 0 0 0 1 1 0 0]\n",
            "True:[1 0 0 0 1 1 0 0]\n",
            "40 + 100 = 140\n",
            "------------\n",
            "iters:7600\n",
            "Loss:0.013193438256466329\n",
            "Pred:[1 1 0 1 1 1 1 0]\n",
            "True:[1 1 0 1 1 1 1 0]\n",
            "104 + 118 = 222\n",
            "------------\n",
            "iters:7700\n",
            "Loss:0.005497346005397309\n",
            "Pred:[0 1 1 1 0 1 0 1]\n",
            "True:[0 1 1 1 0 1 0 1]\n",
            "111 + 6 = 117\n",
            "------------\n",
            "iters:7800\n",
            "Loss:0.007155810042257635\n",
            "Pred:[1 0 1 1 1 0 1 1]\n",
            "True:[1 0 1 1 1 0 1 1]\n",
            "66 + 121 = 187\n",
            "------------\n",
            "iters:7900\n",
            "Loss:0.009675507166105926\n",
            "Pred:[1 0 0 0 1 1 0 0]\n",
            "True:[1 0 0 0 1 1 0 0]\n",
            "110 + 30 = 140\n",
            "------------\n",
            "iters:8000\n",
            "Loss:0.005041834527294626\n",
            "Pred:[0 1 0 1 0 0 0 1]\n",
            "True:[0 1 0 1 0 0 0 1]\n",
            "15 + 66 = 81\n",
            "------------\n",
            "iters:8100\n",
            "Loss:0.00551178126820012\n",
            "Pred:[0 1 1 1 0 1 0 1]\n",
            "True:[0 1 1 1 0 1 0 1]\n",
            "114 + 3 = 117\n",
            "------------\n",
            "iters:8200\n",
            "Loss:0.004969725745333619\n",
            "Pred:[1 0 0 1 0 1 1 1]\n",
            "True:[1 0 0 1 0 1 1 1]\n",
            "30 + 121 = 151\n",
            "------------\n",
            "iters:8300\n",
            "Loss:0.0007016488596578729\n",
            "Pred:[1 0 1 0 0 0 1 0]\n",
            "True:[1 0 1 0 0 0 1 0]\n",
            "117 + 45 = 162\n",
            "------------\n",
            "iters:8400\n",
            "Loss:0.00046781990716916765\n",
            "Pred:[0 1 1 1 1 0 0 0]\n",
            "True:[0 1 1 1 1 0 0 0]\n",
            "69 + 51 = 120\n",
            "------------\n",
            "iters:8500\n",
            "Loss:0.004439410609656961\n",
            "Pred:[1 0 0 1 0 0 0 1]\n",
            "True:[1 0 0 1 0 0 0 1]\n",
            "91 + 54 = 145\n",
            "------------\n",
            "iters:8600\n",
            "Loss:0.005113208295186593\n",
            "Pred:[0 0 1 0 0 0 0 1]\n",
            "True:[0 0 1 0 0 0 0 1]\n",
            "20 + 13 = 33\n",
            "------------\n",
            "iters:8700\n",
            "Loss:0.0004508440808513293\n",
            "Pred:[0 1 0 0 0 1 0 0]\n",
            "True:[0 1 0 0 0 1 0 0]\n",
            "65 + 3 = 68\n",
            "------------\n",
            "iters:8800\n",
            "Loss:0.0037040327280947745\n",
            "Pred:[0 1 1 0 0 0 1 1]\n",
            "True:[0 1 1 0 0 0 1 1]\n",
            "35 + 64 = 99\n",
            "------------\n",
            "iters:8900\n",
            "Loss:0.0031997332263423435\n",
            "Pred:[1 0 0 1 0 1 0 1]\n",
            "True:[1 0 0 1 0 1 0 1]\n",
            "101 + 48 = 149\n",
            "------------\n",
            "iters:9000\n",
            "Loss:0.006843981285488637\n",
            "Pred:[1 0 1 0 1 1 0 0]\n",
            "True:[1 0 1 0 1 1 0 0]\n",
            "112 + 60 = 172\n",
            "------------\n",
            "iters:9100\n",
            "Loss:0.00039853255889058203\n",
            "Pred:[0 0 1 1 1 0 0 0]\n",
            "True:[0 0 1 1 1 0 0 0]\n",
            "39 + 17 = 56\n",
            "------------\n",
            "iters:9200\n",
            "Loss:0.0005816506128034033\n",
            "Pred:[0 1 0 0 1 1 1 0]\n",
            "True:[0 1 0 0 1 1 1 0]\n",
            "29 + 49 = 78\n",
            "------------\n",
            "iters:9300\n",
            "Loss:0.007838031090877499\n",
            "Pred:[1 0 0 0 1 0 1 0]\n",
            "True:[1 0 0 0 1 0 1 0]\n",
            "100 + 38 = 138\n",
            "------------\n",
            "iters:9400\n",
            "Loss:0.0027838324553346935\n",
            "Pred:[1 0 0 1 1 1 1 1]\n",
            "True:[1 0 0 1 1 1 1 1]\n",
            "85 + 74 = 159\n",
            "------------\n",
            "iters:9500\n",
            "Loss:0.00043004976600917525\n",
            "Pred:[1 0 0 1 0 0 0 0]\n",
            "True:[1 0 0 1 0 0 0 0]\n",
            "59 + 85 = 144\n",
            "------------\n",
            "iters:9600\n",
            "Loss:0.0027228128438365105\n",
            "Pred:[1 0 0 0 1 1 1 1]\n",
            "True:[1 0 0 0 1 1 1 1]\n",
            "83 + 60 = 143\n",
            "------------\n",
            "iters:9700\n",
            "Loss:0.0061740955446819255\n",
            "Pred:[0 0 1 1 1 0 1 0]\n",
            "True:[0 0 1 1 1 0 1 0]\n",
            "38 + 20 = 58\n",
            "------------\n",
            "iters:9800\n",
            "Loss:0.003337015181953627\n",
            "Pred:[1 0 1 1 0 1 1 1]\n",
            "True:[1 0 1 1 0 1 1 1]\n",
            "106 + 77 = 183\n",
            "------------\n",
            "iters:9900\n",
            "Loss:0.0028061426725525026\n",
            "Pred:[0 1 1 0 0 0 1 1]\n",
            "True:[0 1 1 0 0 0 1 1]\n",
            "52 + 47 = 99\n",
            "------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52ge5qD5YJjN"
      },
      "source": [
        "4000回くらいでほぼ精度は達成されている。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7zQEPrtP4OP"
      },
      "source": [
        "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "## [try] weight_init_stdやlearning_rate, hidden_layer_sizeを変更してみよう\n",
        "\n",
        "\n",
        "## [try] 重みの初期化方法を変更してみよう\n",
        "Xavier, He\n",
        "\n",
        "## [try] 中間層の活性化関数を変更してみよう\n",
        "ReLU(勾配爆発を確認しよう)<br>\n",
        "tanh(numpyにtanhが用意されている。導関数をd_tanhとして作成しよう)\n",
        "\n",
        "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A1SE4uYaPKS"
      },
      "source": [
        "#重みの初期化をHeにしてみる"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OlZbTLwIaO3J",
        "outputId": "fa550fc4-29ce-4d65-d5f6-46dc9a1ae797"
      },
      "source": [
        "import numpy as np\n",
        "from common import functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# def d_tanh(x):\n",
        "\n",
        "\n",
        "\n",
        "# データを用意\n",
        "# 2進数の桁数\n",
        "binary_dim = 8\n",
        "# 最大値 + 1\n",
        "largest_number = pow(2, binary_dim)\n",
        "# largest_numberまで2進数を用意\n",
        "binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
        "\n",
        "input_layer_size = 2\n",
        "hidden_layer_size = 16\n",
        "output_layer_size = 1\n",
        "\n",
        "weight_init_std = 1\n",
        "learning_rate = 0.1\n",
        "\n",
        "iters_num = 10000\n",
        "plot_interval = 100\n",
        "\n",
        "# ウェイト初期化 (バイアスは簡単のため省略)\n",
        "W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n",
        "W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n",
        "W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n",
        "\n",
        "# Xavier\n",
        "\n",
        "\n",
        "# He\n",
        "W_in = np.random.randn(input_layer_size, hidden_layer_size) / np.sqrt(input_layer_size) * np.sqrt(2)\n",
        "W_out = np.random.randn(hidden_layer_size, output_layer_size) / np.sqrt(hidden_layer_size) * np.sqrt(2)\n",
        "W = np.random.randn(hidden_layer_size, hidden_layer_size) / np.sqrt(hidden_layer_size) * np.sqrt(2)\n",
        "\n",
        "\n",
        "\n",
        "# 勾配\n",
        "W_in_grad = np.zeros_like(W_in)\n",
        "W_out_grad = np.zeros_like(W_out)\n",
        "W_grad = np.zeros_like(W)\n",
        "\n",
        "u = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "z = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "y = np.zeros((output_layer_size, binary_dim))\n",
        "\n",
        "delta_out = np.zeros((output_layer_size, binary_dim))\n",
        "delta = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "\n",
        "all_losses = []\n",
        "\n",
        "for i in range(iters_num):\n",
        "    \n",
        "    # A, B初期化 (a + b = d)\n",
        "    a_int = np.random.randint(largest_number/2)\n",
        "    a_bin = binary[a_int] # binary encoding\n",
        "    b_int = np.random.randint(largest_number/2)\n",
        "    b_bin = binary[b_int] # binary encoding\n",
        "    \n",
        "    # 正解データ\n",
        "    d_int = a_int + b_int\n",
        "    d_bin = binary[d_int]\n",
        "    \n",
        "    # 出力バイナリ\n",
        "    out_bin = np.zeros_like(d_bin)\n",
        "    \n",
        "    # 時系列全体の誤差\n",
        "    all_loss = 0    \n",
        "    \n",
        "    # 時系列ループ\n",
        "    for t in range(binary_dim):\n",
        "        # 入力値\n",
        "        X = np.array([a_bin[ - t - 1], b_bin[ - t - 1]]).reshape(1, -1)\n",
        "        # 時刻tにおける正解データ\n",
        "        dd = np.array([d_bin[binary_dim - t - 1]])\n",
        "        \n",
        "        u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n",
        "        z[:,t+1] = functions.sigmoid(u[:,t+1])\n",
        "\n",
        "        y[:,t] = functions.sigmoid(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
        "\n",
        "\n",
        "        #誤差\n",
        "        loss = functions.mean_squared_error(dd, y[:,t])\n",
        "        \n",
        "        delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * functions.d_sigmoid(y[:,t])        \n",
        "        \n",
        "        all_loss += loss\n",
        "\n",
        "        out_bin[binary_dim - t - 1] = np.round(y[:,t])\n",
        "    \n",
        "    \n",
        "    for t in range(binary_dim)[::-1]:\n",
        "        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n",
        "\n",
        "        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_sigmoid(u[:,t+1])\n",
        "\n",
        "        # 勾配更新\n",
        "        W_out_grad += np.dot(z[:,t+1].reshape(-1,1), delta_out[:,t].reshape(-1,1))\n",
        "        W_grad += np.dot(z[:,t].reshape(-1,1), delta[:,t].reshape(1,-1))\n",
        "        W_in_grad += np.dot(X.T, delta[:,t].reshape(1,-1))\n",
        "    \n",
        "    # 勾配適用\n",
        "    W_in -= learning_rate * W_in_grad\n",
        "    W_out -= learning_rate * W_out_grad\n",
        "    W -= learning_rate * W_grad\n",
        "    \n",
        "    W_in_grad *= 0\n",
        "    W_out_grad *= 0\n",
        "    W_grad *= 0\n",
        "    \n",
        "\n",
        "    if(i % plot_interval == 0):\n",
        "        all_losses.append(all_loss)        \n",
        "        print(\"iters:\" + str(i))\n",
        "        print(\"Loss:\" + str(all_loss))\n",
        "        print(\"Pred:\" + str(out_bin))\n",
        "        print(\"True:\" + str(d_bin))\n",
        "        out_int = 0\n",
        "        for index,x in enumerate(reversed(out_bin)):\n",
        "            out_int += x * pow(2, index)\n",
        "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out_int))\n",
        "        print(\"------------\")\n",
        "\n",
        "lists = range(0, iters_num, plot_interval)\n",
        "plt.plot(lists, all_losses, label=\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iters:0\n",
            "Loss:1.0950864529235749\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 1 1 0 0 1]\n",
            "16 + 105 = 255\n",
            "------------\n",
            "iters:100\n",
            "Loss:0.9058363583470724\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 0 0 1 0]\n",
            "77 + 117 = 0\n",
            "------------\n",
            "iters:200\n",
            "Loss:0.9604544911204514\n",
            "Pred:[1 1 0 1 1 1 1 1]\n",
            "True:[0 1 0 1 1 1 0 1]\n",
            "60 + 33 = 223\n",
            "------------\n",
            "iters:300\n",
            "Loss:1.0398273635302882\n",
            "Pred:[0 0 0 0 0 0 0 1]\n",
            "True:[1 0 0 0 1 1 0 0]\n",
            "45 + 95 = 1\n",
            "------------\n",
            "iters:400\n",
            "Loss:1.0454404980461645\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[0 1 0 1 0 1 1 0]\n",
            "43 + 43 = 255\n",
            "------------\n",
            "iters:500\n",
            "Loss:0.9901901375204372\n",
            "Pred:[0 0 0 0 0 0 0 1]\n",
            "True:[0 0 1 0 0 1 1 1]\n",
            "39 + 0 = 1\n",
            "------------\n",
            "iters:600\n",
            "Loss:0.9476139425520116\n",
            "Pred:[0 1 1 0 0 1 0 0]\n",
            "True:[0 1 0 0 0 0 0 0]\n",
            "37 + 27 = 100\n",
            "------------\n",
            "iters:700\n",
            "Loss:0.9643536667083472\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 1 0 1]\n",
            "76 + 57 = 0\n",
            "------------\n",
            "iters:800\n",
            "Loss:0.9756138547885322\n",
            "Pred:[1 1 0 1 1 1 1 1]\n",
            "True:[0 1 1 1 1 0 0 1]\n",
            "12 + 109 = 223\n",
            "------------\n",
            "iters:900\n",
            "Loss:0.9170430773583333\n",
            "Pred:[0 0 1 1 1 1 0 1]\n",
            "True:[0 0 0 0 1 1 0 0]\n",
            "10 + 2 = 61\n",
            "------------\n",
            "iters:1000\n",
            "Loss:1.0262854703516688\n",
            "Pred:[1 1 1 0 1 0 0 0]\n",
            "True:[1 1 0 0 1 1 1 0]\n",
            "126 + 80 = 232\n",
            "------------\n",
            "iters:1100\n",
            "Loss:1.0090802898732165\n",
            "Pred:[0 0 0 0 1 1 0 1]\n",
            "True:[0 1 1 1 0 0 1 1]\n",
            "38 + 77 = 13\n",
            "------------\n",
            "iters:1200\n",
            "Loss:0.9908593534513177\n",
            "Pred:[0 1 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 1 1 1 0]\n",
            "77 + 33 = 64\n",
            "------------\n",
            "iters:1300\n",
            "Loss:0.9033916932797434\n",
            "Pred:[0 0 0 1 0 1 0 1]\n",
            "True:[0 0 1 0 0 1 0 1]\n",
            "26 + 11 = 21\n",
            "------------\n",
            "iters:1400\n",
            "Loss:0.9319196192395827\n",
            "Pred:[1 1 0 0 1 1 1 1]\n",
            "True:[0 1 1 0 1 0 1 1]\n",
            "37 + 70 = 207\n",
            "------------\n",
            "iters:1500\n",
            "Loss:1.122715832025838\n",
            "Pred:[0 1 1 0 0 0 0 0]\n",
            "True:[1 0 0 0 1 1 1 0]\n",
            "113 + 29 = 96\n",
            "------------\n",
            "iters:1600\n",
            "Loss:0.836410140263527\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 1 1 1 0 1]\n",
            "32 + 93 = 255\n",
            "------------\n",
            "iters:1700\n",
            "Loss:1.069881154985718\n",
            "Pred:[0 1 0 0 1 0 0 0]\n",
            "True:[0 1 1 1 0 0 1 0]\n",
            "44 + 70 = 72\n",
            "------------\n",
            "iters:1800\n",
            "Loss:0.7572156675666883\n",
            "Pred:[1 1 1 0 0 1 1 1]\n",
            "True:[0 1 1 0 0 0 1 1]\n",
            "17 + 82 = 231\n",
            "------------\n",
            "iters:1900\n",
            "Loss:0.7846328096901275\n",
            "Pred:[0 0 0 0 0 0 0 1]\n",
            "True:[0 0 1 0 0 0 1 1]\n",
            "6 + 29 = 1\n",
            "------------\n",
            "iters:2000\n",
            "Loss:0.9461460826796206\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[1 0 0 1 1 1 1 0]\n",
            "113 + 45 = 255\n",
            "------------\n",
            "iters:2100\n",
            "Loss:0.7955579327453178\n",
            "Pred:[0 0 0 0 1 1 0 1]\n",
            "True:[0 1 0 1 1 1 0 1]\n",
            "71 + 22 = 13\n",
            "------------\n",
            "iters:2200\n",
            "Loss:0.7084891864321408\n",
            "Pred:[0 1 1 1 1 1 1 1]\n",
            "True:[0 0 1 1 1 0 1 1]\n",
            "36 + 23 = 127\n",
            "------------\n",
            "iters:2300\n",
            "Loss:0.892003437521146\n",
            "Pred:[0 1 0 0 0 0 0 1]\n",
            "True:[0 0 1 1 1 1 0 1]\n",
            "5 + 56 = 65\n",
            "------------\n",
            "iters:2400\n",
            "Loss:0.7116003169508308\n",
            "Pred:[1 1 0 0 0 1 0 0]\n",
            "True:[1 1 0 1 1 1 0 1]\n",
            "122 + 99 = 196\n",
            "------------\n",
            "iters:2500\n",
            "Loss:0.761786693774289\n",
            "Pred:[1 0 0 0 0 1 0 1]\n",
            "True:[1 0 1 0 0 0 0 1]\n",
            "66 + 95 = 133\n",
            "------------\n",
            "iters:2600\n",
            "Loss:0.7605499410056576\n",
            "Pred:[1 1 1 0 0 1 1 1]\n",
            "True:[1 0 1 0 0 1 1 1]\n",
            "117 + 50 = 231\n",
            "------------\n",
            "iters:2700\n",
            "Loss:0.4918310581039354\n",
            "Pred:[1 1 0 1 1 1 0 1]\n",
            "True:[1 0 0 1 1 1 0 1]\n",
            "47 + 110 = 221\n",
            "------------\n",
            "iters:2800\n",
            "Loss:0.5398084517793147\n",
            "Pred:[1 0 1 1 0 1 1 1]\n",
            "True:[1 0 1 0 0 1 1 1]\n",
            "79 + 88 = 183\n",
            "------------\n",
            "iters:2900\n",
            "Loss:0.8227531379809396\n",
            "Pred:[1 0 0 1 0 1 0 1]\n",
            "True:[0 1 1 1 0 1 0 1]\n",
            "0 + 117 = 149\n",
            "------------\n",
            "iters:3000\n",
            "Loss:0.45619947700223007\n",
            "Pred:[0 1 1 1 1 0 1 1]\n",
            "True:[0 1 1 1 1 0 1 1]\n",
            "25 + 98 = 123\n",
            "------------\n",
            "iters:3100\n",
            "Loss:0.29163327818764734\n",
            "Pred:[1 0 0 1 1 0 1 0]\n",
            "True:[1 0 0 1 1 0 1 0]\n",
            "85 + 69 = 154\n",
            "------------\n",
            "iters:3200\n",
            "Loss:0.32199944386888796\n",
            "Pred:[0 1 1 1 0 1 1 0]\n",
            "True:[0 1 1 1 0 1 0 0]\n",
            "59 + 57 = 118\n",
            "------------\n",
            "iters:3300\n",
            "Loss:0.7407372888719466\n",
            "Pred:[1 0 1 0 0 1 0 0]\n",
            "True:[1 1 0 0 0 1 0 0]\n",
            "108 + 88 = 164\n",
            "------------\n",
            "iters:3400\n",
            "Loss:0.49044941457718416\n",
            "Pred:[1 0 0 1 1 1 1 0]\n",
            "True:[1 0 0 1 1 1 1 0]\n",
            "54 + 104 = 158\n",
            "------------\n",
            "iters:3500\n",
            "Loss:0.40785287249174407\n",
            "Pred:[0 1 1 0 1 1 1 0]\n",
            "True:[0 1 1 0 1 1 1 0]\n",
            "45 + 65 = 110\n",
            "------------\n",
            "iters:3600\n",
            "Loss:0.3199039012784476\n",
            "Pred:[0 0 0 1 0 1 1 1]\n",
            "True:[0 0 0 1 0 1 1 1]\n",
            "23 + 0 = 23\n",
            "------------\n",
            "iters:3700\n",
            "Loss:0.08772386811225175\n",
            "Pred:[0 1 1 1 0 1 0 1]\n",
            "True:[0 1 1 1 0 1 0 1]\n",
            "76 + 41 = 117\n",
            "------------\n",
            "iters:3800\n",
            "Loss:0.3860265875716238\n",
            "Pred:[1 0 1 0 1 0 0 0]\n",
            "True:[1 0 1 0 1 0 0 0]\n",
            "126 + 42 = 168\n",
            "------------\n",
            "iters:3900\n",
            "Loss:0.15540013713165673\n",
            "Pred:[0 0 1 0 0 0 0 1]\n",
            "True:[0 0 1 0 0 0 0 1]\n",
            "25 + 8 = 33\n",
            "------------\n",
            "iters:4000\n",
            "Loss:0.08857952109636162\n",
            "Pred:[1 0 1 1 1 1 1 0]\n",
            "True:[1 0 1 1 1 1 1 0]\n",
            "107 + 83 = 190\n",
            "------------\n",
            "iters:4100\n",
            "Loss:0.06967389280227264\n",
            "Pred:[0 0 1 0 0 0 1 0]\n",
            "True:[0 0 1 0 0 0 1 0]\n",
            "16 + 18 = 34\n",
            "------------\n",
            "iters:4200\n",
            "Loss:0.07838919368427098\n",
            "Pred:[0 1 0 0 1 1 0 0]\n",
            "True:[0 1 0 0 1 1 0 0]\n",
            "56 + 20 = 76\n",
            "------------\n",
            "iters:4300\n",
            "Loss:0.05376094371621482\n",
            "Pred:[0 0 0 0 1 1 0 0]\n",
            "True:[0 0 0 0 1 1 0 0]\n",
            "4 + 8 = 12\n",
            "------------\n",
            "iters:4400\n",
            "Loss:0.0314887351182297\n",
            "Pred:[1 0 1 1 0 1 1 0]\n",
            "True:[1 0 1 1 0 1 1 0]\n",
            "75 + 107 = 182\n",
            "------------\n",
            "iters:4500\n",
            "Loss:0.12043750822261508\n",
            "Pred:[1 0 1 1 1 1 1 0]\n",
            "True:[1 0 1 1 1 1 1 0]\n",
            "72 + 118 = 190\n",
            "------------\n",
            "iters:4600\n",
            "Loss:0.024687033579289475\n",
            "Pred:[1 0 1 1 1 0 0 1]\n",
            "True:[1 0 1 1 1 0 0 1]\n",
            "106 + 79 = 185\n",
            "------------\n",
            "iters:4700\n",
            "Loss:0.04489067041818342\n",
            "Pred:[0 0 1 1 1 0 1 1]\n",
            "True:[0 0 1 1 1 0 1 1]\n",
            "11 + 48 = 59\n",
            "------------\n",
            "iters:4800\n",
            "Loss:0.04116053497172285\n",
            "Pred:[0 1 0 1 0 0 1 1]\n",
            "True:[0 1 0 1 0 0 1 1]\n",
            "52 + 31 = 83\n",
            "------------\n",
            "iters:4900\n",
            "Loss:0.06372166869286144\n",
            "Pred:[0 1 0 0 0 0 0 1]\n",
            "True:[0 1 0 0 0 0 0 1]\n",
            "27 + 38 = 65\n",
            "------------\n",
            "iters:5000\n",
            "Loss:0.014695871658722048\n",
            "Pred:[0 1 0 0 0 0 1 0]\n",
            "True:[0 1 0 0 0 0 1 0]\n",
            "37 + 29 = 66\n",
            "------------\n",
            "iters:5100\n",
            "Loss:0.02450332594153026\n",
            "Pred:[0 1 1 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 0 0 0]\n",
            "37 + 59 = 96\n",
            "------------\n",
            "iters:5200\n",
            "Loss:0.0303735146533899\n",
            "Pred:[0 1 1 1 1 0 1 0]\n",
            "True:[0 1 1 1 1 0 1 0]\n",
            "20 + 102 = 122\n",
            "------------\n",
            "iters:5300\n",
            "Loss:0.027098154221684257\n",
            "Pred:[1 0 1 0 1 1 1 1]\n",
            "True:[1 0 1 0 1 1 1 1]\n",
            "75 + 100 = 175\n",
            "------------\n",
            "iters:5400\n",
            "Loss:0.011297029886556655\n",
            "Pred:[0 1 0 0 1 1 1 1]\n",
            "True:[0 1 0 0 1 1 1 1]\n",
            "16 + 63 = 79\n",
            "------------\n",
            "iters:5500\n",
            "Loss:0.02547331116735455\n",
            "Pred:[1 0 0 1 1 0 1 0]\n",
            "True:[1 0 0 1 1 0 1 0]\n",
            "36 + 118 = 154\n",
            "------------\n",
            "iters:5600\n",
            "Loss:0.013311579500565909\n",
            "Pred:[0 0 1 0 0 0 0 1]\n",
            "True:[0 0 1 0 0 0 0 1]\n",
            "17 + 16 = 33\n",
            "------------\n",
            "iters:5700\n",
            "Loss:0.011201276044305965\n",
            "Pred:[1 0 0 0 1 0 1 1]\n",
            "True:[1 0 0 0 1 0 1 1]\n",
            "48 + 91 = 139\n",
            "------------\n",
            "iters:5800\n",
            "Loss:0.020797272983940025\n",
            "Pred:[0 1 1 0 1 1 0 0]\n",
            "True:[0 1 1 0 1 1 0 0]\n",
            "8 + 100 = 108\n",
            "------------\n",
            "iters:5900\n",
            "Loss:0.011086589327436953\n",
            "Pred:[1 0 0 0 0 1 1 0]\n",
            "True:[1 0 0 0 0 1 1 0]\n",
            "121 + 13 = 134\n",
            "------------\n",
            "iters:6000\n",
            "Loss:0.007797726852850953\n",
            "Pred:[0 1 0 1 1 1 0 1]\n",
            "True:[0 1 0 1 1 1 0 1]\n",
            "56 + 37 = 93\n",
            "------------\n",
            "iters:6100\n",
            "Loss:0.001518906695406836\n",
            "Pred:[0 1 0 1 0 0 1 0]\n",
            "True:[0 1 0 1 0 0 1 0]\n",
            "25 + 57 = 82\n",
            "------------\n",
            "iters:6200\n",
            "Loss:0.012770261933089246\n",
            "Pred:[1 0 0 0 0 1 1 1]\n",
            "True:[1 0 0 0 0 1 1 1]\n",
            "63 + 72 = 135\n",
            "------------\n",
            "iters:6300\n",
            "Loss:0.0064304626508632405\n",
            "Pred:[1 0 1 0 0 1 1 1]\n",
            "True:[1 0 1 0 0 1 1 1]\n",
            "78 + 89 = 167\n",
            "------------\n",
            "iters:6400\n",
            "Loss:0.010052394838103763\n",
            "Pred:[0 0 1 0 0 0 0 1]\n",
            "True:[0 0 1 0 0 0 0 1]\n",
            "23 + 10 = 33\n",
            "------------\n",
            "iters:6500\n",
            "Loss:0.007773763386151723\n",
            "Pred:[1 0 1 0 1 1 0 1]\n",
            "True:[1 0 1 0 1 1 0 1]\n",
            "87 + 86 = 173\n",
            "------------\n",
            "iters:6600\n",
            "Loss:0.00384484256116824\n",
            "Pred:[0 1 1 0 0 0 0 1]\n",
            "True:[0 1 1 0 0 0 0 1]\n",
            "42 + 55 = 97\n",
            "------------\n",
            "iters:6700\n",
            "Loss:0.006839291499222516\n",
            "Pred:[1 0 1 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 0 0 0]\n",
            "125 + 35 = 160\n",
            "------------\n",
            "iters:6800\n",
            "Loss:0.004928418535249851\n",
            "Pred:[0 1 1 1 1 0 1 1]\n",
            "True:[0 1 1 1 1 0 1 1]\n",
            "94 + 29 = 123\n",
            "------------\n",
            "iters:6900\n",
            "Loss:0.008843775410556413\n",
            "Pred:[0 1 0 0 1 1 0 1]\n",
            "True:[0 1 0 0 1 1 0 1]\n",
            "63 + 14 = 77\n",
            "------------\n",
            "iters:7000\n",
            "Loss:0.0007091286422250743\n",
            "Pred:[0 1 1 1 0 1 1 0]\n",
            "True:[0 1 1 1 0 1 1 0]\n",
            "17 + 101 = 118\n",
            "------------\n",
            "iters:7100\n",
            "Loss:0.0013847256716934516\n",
            "Pred:[0 0 1 1 1 0 1 0]\n",
            "True:[0 0 1 1 1 0 1 0]\n",
            "19 + 39 = 58\n",
            "------------\n",
            "iters:7200\n",
            "Loss:0.005417460551903564\n",
            "Pred:[0 1 1 0 0 1 0 1]\n",
            "True:[0 1 1 0 0 1 0 1]\n",
            "73 + 28 = 101\n",
            "------------\n",
            "iters:7300\n",
            "Loss:0.0023860505880302246\n",
            "Pred:[0 1 0 1 0 1 0 1]\n",
            "True:[0 1 0 1 0 1 0 1]\n",
            "72 + 13 = 85\n",
            "------------\n",
            "iters:7400\n",
            "Loss:0.008444350926170031\n",
            "Pred:[1 0 1 0 1 1 1 0]\n",
            "True:[1 0 1 0 1 1 1 0]\n",
            "54 + 120 = 174\n",
            "------------\n",
            "iters:7500\n",
            "Loss:0.013199115608707773\n",
            "Pred:[0 0 1 0 0 0 0 0]\n",
            "True:[0 0 1 0 0 0 0 0]\n",
            "30 + 2 = 32\n",
            "------------\n",
            "iters:7600\n",
            "Loss:0.001065309637256905\n",
            "Pred:[1 0 1 0 1 1 0 0]\n",
            "True:[1 0 1 0 1 1 0 0]\n",
            "121 + 51 = 172\n",
            "------------\n",
            "iters:7700\n",
            "Loss:0.0029418709802899417\n",
            "Pred:[1 0 1 0 0 0 1 1]\n",
            "True:[1 0 1 0 0 0 1 1]\n",
            "98 + 65 = 163\n",
            "------------\n",
            "iters:7800\n",
            "Loss:0.004590023047564717\n",
            "Pred:[0 1 1 1 1 0 0 1]\n",
            "True:[0 1 1 1 1 0 0 1]\n",
            "105 + 16 = 121\n",
            "------------\n",
            "iters:7900\n",
            "Loss:0.0065964519947497744\n",
            "Pred:[0 1 1 1 0 1 0 0]\n",
            "True:[0 1 1 1 0 1 0 0]\n",
            "28 + 88 = 116\n",
            "------------\n",
            "iters:8000\n",
            "Loss:0.0024729985791485266\n",
            "Pred:[0 1 1 0 1 1 0 1]\n",
            "True:[0 1 1 0 1 1 0 1]\n",
            "78 + 31 = 109\n",
            "------------\n",
            "iters:8100\n",
            "Loss:0.0009052903942162804\n",
            "Pred:[1 0 0 0 0 0 1 0]\n",
            "True:[1 0 0 0 0 0 1 0]\n",
            "19 + 111 = 130\n",
            "------------\n",
            "iters:8200\n",
            "Loss:0.0009771375474120474\n",
            "Pred:[0 1 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 0 0 0 0]\n",
            "33 + 31 = 64\n",
            "------------\n",
            "iters:8300\n",
            "Loss:0.004849600989078842\n",
            "Pred:[1 1 0 0 1 1 1 0]\n",
            "True:[1 1 0 0 1 1 1 0]\n",
            "98 + 108 = 206\n",
            "------------\n",
            "iters:8400\n",
            "Loss:0.00094421249717391\n",
            "Pred:[1 0 1 1 0 1 0 0]\n",
            "True:[1 0 1 1 0 1 0 0]\n",
            "127 + 53 = 180\n",
            "------------\n",
            "iters:8500\n",
            "Loss:0.0002929065409117714\n",
            "Pred:[0 1 1 1 1 1 1 0]\n",
            "True:[0 1 1 1 1 1 1 0]\n",
            "107 + 19 = 126\n",
            "------------\n",
            "iters:8600\n",
            "Loss:0.0050218524317149765\n",
            "Pred:[1 1 0 1 0 0 0 0]\n",
            "True:[1 1 0 1 0 0 0 0]\n",
            "124 + 84 = 208\n",
            "------------\n",
            "iters:8700\n",
            "Loss:0.00549353580814733\n",
            "Pred:[0 1 1 1 1 1 0 0]\n",
            "True:[0 1 1 1 1 1 0 0]\n",
            "58 + 66 = 124\n",
            "------------\n",
            "iters:8800\n",
            "Loss:0.000384323978745426\n",
            "Pred:[1 1 0 1 1 0 1 0]\n",
            "True:[1 1 0 1 1 0 1 0]\n",
            "99 + 119 = 218\n",
            "------------\n",
            "iters:8900\n",
            "Loss:0.001228811655130134\n",
            "Pred:[0 0 0 1 1 1 0 1]\n",
            "True:[0 0 0 1 1 1 0 1]\n",
            "12 + 17 = 29\n",
            "------------\n",
            "iters:9000\n",
            "Loss:0.00179975218932984\n",
            "Pred:[1 0 0 0 0 0 1 1]\n",
            "True:[1 0 0 0 0 0 1 1]\n",
            "76 + 55 = 131\n",
            "------------\n",
            "iters:9100\n",
            "Loss:0.004441824055756992\n",
            "Pred:[1 1 0 0 0 0 1 0]\n",
            "True:[1 1 0 0 0 0 1 0]\n",
            "126 + 68 = 194\n",
            "------------\n",
            "iters:9200\n",
            "Loss:0.0004952742601858261\n",
            "Pred:[1 1 0 1 0 0 0 0]\n",
            "True:[1 1 0 1 0 0 0 0]\n",
            "105 + 103 = 208\n",
            "------------\n",
            "iters:9300\n",
            "Loss:0.00024997741695816474\n",
            "Pred:[0 0 1 0 1 1 0 0]\n",
            "True:[0 0 1 0 1 1 0 0]\n",
            "13 + 31 = 44\n",
            "------------\n",
            "iters:9400\n",
            "Loss:0.0006688522139551252\n",
            "Pred:[1 1 1 0 0 1 0 0]\n",
            "True:[1 1 1 0 0 1 0 0]\n",
            "125 + 103 = 228\n",
            "------------\n",
            "iters:9500\n",
            "Loss:0.0011596564100310916\n",
            "Pred:[0 1 1 0 0 0 0 1]\n",
            "True:[0 1 1 0 0 0 0 1]\n",
            "26 + 71 = 97\n",
            "------------\n",
            "iters:9600\n",
            "Loss:0.00017370671951232817\n",
            "Pred:[0 1 0 1 1 1 0 0]\n",
            "True:[0 1 0 1 1 1 0 0]\n",
            "21 + 71 = 92\n",
            "------------\n",
            "iters:9700\n",
            "Loss:0.001948298585319294\n",
            "Pred:[0 1 0 0 1 1 1 1]\n",
            "True:[0 1 0 0 1 1 1 1]\n",
            "75 + 4 = 79\n",
            "------------\n",
            "iters:9800\n",
            "Loss:0.00035668672453564886\n",
            "Pred:[1 0 1 0 1 1 0 0]\n",
            "True:[1 0 1 0 1 1 0 0]\n",
            "113 + 59 = 172\n",
            "------------\n",
            "iters:9900\n",
            "Loss:0.003271316373276008\n",
            "Pred:[1 0 1 0 1 1 1 0]\n",
            "True:[1 0 1 0 1 1 1 0]\n",
            "122 + 52 = 174\n",
            "------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQiiT6J4arnL"
      },
      "source": [
        "ちょっとゆっくり。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9_jn5pIrYFXE",
        "outputId": "11d3c55c-76bd-46e5-b0ca-8a6e64cabc3d"
      },
      "source": [
        "#[try] weight_init_stdやlearning_rate, hidden_layer_sizeを変更してみよう\n",
        "#lrを0.1から0.01にしてゆっくりにしてみる。\n",
        "import numpy as np\n",
        "from common import functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# def d_tanh(x):\n",
        "\n",
        "\n",
        "\n",
        "# データを用意\n",
        "# 2進数の桁数\n",
        "binary_dim = 8\n",
        "# 最大値 + 1\n",
        "largest_number = pow(2, binary_dim)\n",
        "# largest_numberまで2進数を用意\n",
        "binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
        "\n",
        "input_layer_size = 2\n",
        "hidden_layer_size = 16\n",
        "output_layer_size = 1\n",
        "\n",
        "weight_init_std = 1\n",
        "learning_rate = 0.01\n",
        "\n",
        "iters_num = 10000\n",
        "plot_interval = 100\n",
        "\n",
        "# ウェイト初期化 (バイアスは簡単のため省略)\n",
        "W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n",
        "W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n",
        "W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n",
        "\n",
        "# Xavier\n",
        "\n",
        "\n",
        "# He\n",
        "\n",
        "\n",
        "\n",
        "# 勾配\n",
        "W_in_grad = np.zeros_like(W_in)\n",
        "W_out_grad = np.zeros_like(W_out)\n",
        "W_grad = np.zeros_like(W)\n",
        "\n",
        "u = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "z = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "y = np.zeros((output_layer_size, binary_dim))\n",
        "\n",
        "delta_out = np.zeros((output_layer_size, binary_dim))\n",
        "delta = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "\n",
        "all_losses = []\n",
        "\n",
        "for i in range(iters_num):\n",
        "    \n",
        "    # A, B初期化 (a + b = d)\n",
        "    a_int = np.random.randint(largest_number/2)\n",
        "    a_bin = binary[a_int] # binary encoding\n",
        "    b_int = np.random.randint(largest_number/2)\n",
        "    b_bin = binary[b_int] # binary encoding\n",
        "    \n",
        "    # 正解データ\n",
        "    d_int = a_int + b_int\n",
        "    d_bin = binary[d_int]\n",
        "    \n",
        "    # 出力バイナリ\n",
        "    out_bin = np.zeros_like(d_bin)\n",
        "    \n",
        "    # 時系列全体の誤差\n",
        "    all_loss = 0    \n",
        "    \n",
        "    # 時系列ループ\n",
        "    for t in range(binary_dim):\n",
        "        # 入力値\n",
        "        X = np.array([a_bin[ - t - 1], b_bin[ - t - 1]]).reshape(1, -1)\n",
        "        # 時刻tにおける正解データ\n",
        "        dd = np.array([d_bin[binary_dim - t - 1]])\n",
        "        \n",
        "        u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n",
        "        z[:,t+1] = functions.sigmoid(u[:,t+1])\n",
        "\n",
        "        y[:,t] = functions.sigmoid(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
        "\n",
        "\n",
        "        #誤差\n",
        "        loss = functions.mean_squared_error(dd, y[:,t])\n",
        "        \n",
        "        delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * functions.d_sigmoid(y[:,t])        \n",
        "        \n",
        "        all_loss += loss\n",
        "\n",
        "        out_bin[binary_dim - t - 1] = np.round(y[:,t])\n",
        "    \n",
        "    \n",
        "    for t in range(binary_dim)[::-1]:\n",
        "        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n",
        "\n",
        "        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_sigmoid(u[:,t+1])\n",
        "\n",
        "        # 勾配更新\n",
        "        W_out_grad += np.dot(z[:,t+1].reshape(-1,1), delta_out[:,t].reshape(-1,1))\n",
        "        W_grad += np.dot(z[:,t].reshape(-1,1), delta[:,t].reshape(1,-1))\n",
        "        W_in_grad += np.dot(X.T, delta[:,t].reshape(1,-1))\n",
        "    \n",
        "    # 勾配適用\n",
        "    W_in -= learning_rate * W_in_grad\n",
        "    W_out -= learning_rate * W_out_grad\n",
        "    W -= learning_rate * W_grad\n",
        "    \n",
        "    W_in_grad *= 0\n",
        "    W_out_grad *= 0\n",
        "    W_grad *= 0\n",
        "    \n",
        "\n",
        "    if(i % plot_interval == 0):\n",
        "        all_losses.append(all_loss)        \n",
        "        print(\"iters:\" + str(i))\n",
        "        print(\"Loss:\" + str(all_loss))\n",
        "        print(\"Pred:\" + str(out_bin))\n",
        "        print(\"True:\" + str(d_bin))\n",
        "        out_int = 0\n",
        "        for index,x in enumerate(reversed(out_bin)):\n",
        "            out_int += x * pow(2, index)\n",
        "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out_int))\n",
        "        print(\"------------\")\n",
        "\n",
        "lists = range(0, iters_num, plot_interval)\n",
        "plt.plot(lists, all_losses, label=\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iters:0\n",
            "Loss:1.5793331473591252\n",
            "Pred:[1 1 1 1 1 0 1 0]\n",
            "True:[1 0 0 0 0 1 1 1]\n",
            "120 + 15 = 250\n",
            "------------\n",
            "iters:100\n",
            "Loss:1.0842199667010266\n",
            "Pred:[1 0 1 0 0 0 1 1]\n",
            "True:[1 1 0 0 0 1 1 1]\n",
            "107 + 92 = 163\n",
            "------------\n",
            "iters:200\n",
            "Loss:0.9974492867198046\n",
            "Pred:[1 1 0 1 1 0 0 1]\n",
            "True:[0 1 1 1 0 0 0 0]\n",
            "74 + 38 = 217\n",
            "------------\n",
            "iters:300\n",
            "Loss:0.9318828950898397\n",
            "Pred:[1 0 1 0 1 0 1 1]\n",
            "True:[1 0 0 1 1 0 1 0]\n",
            "74 + 80 = 171\n",
            "------------\n",
            "iters:400\n",
            "Loss:1.1211299879824712\n",
            "Pred:[1 0 0 1 0 1 1 1]\n",
            "True:[1 1 1 0 1 0 0 0]\n",
            "112 + 120 = 151\n",
            "------------\n",
            "iters:500\n",
            "Loss:1.1338195063976486\n",
            "Pred:[1 0 0 1 0 1 0 1]\n",
            "True:[1 0 1 0 1 0 0 1]\n",
            "63 + 106 = 149\n",
            "------------\n",
            "iters:600\n",
            "Loss:0.9615269699822284\n",
            "Pred:[1 0 1 1 0 0 0 1]\n",
            "True:[0 1 1 1 0 0 0 0]\n",
            "34 + 78 = 177\n",
            "------------\n",
            "iters:700\n",
            "Loss:0.9804486737406455\n",
            "Pred:[1 0 1 0 1 0 0 1]\n",
            "True:[1 0 0 0 1 1 1 1]\n",
            "57 + 86 = 169\n",
            "------------\n",
            "iters:800\n",
            "Loss:0.9000327249981628\n",
            "Pred:[1 0 1 0 1 1 1 0]\n",
            "True:[0 1 1 1 1 1 1 0]\n",
            "43 + 83 = 174\n",
            "------------\n",
            "iters:900\n",
            "Loss:1.0234878081382734\n",
            "Pred:[1 0 0 0 1 0 1 0]\n",
            "True:[1 1 1 0 0 0 0 0]\n",
            "111 + 113 = 138\n",
            "------------\n",
            "iters:1000\n",
            "Loss:0.9568008584946601\n",
            "Pred:[1 0 0 0 0 0 1 0]\n",
            "True:[1 1 1 0 1 0 1 0]\n",
            "113 + 121 = 130\n",
            "------------\n",
            "iters:1100\n",
            "Loss:1.0462702083256668\n",
            "Pred:[0 1 0 1 0 0 1 0]\n",
            "True:[0 0 1 1 1 1 1 0]\n",
            "19 + 43 = 82\n",
            "------------\n",
            "iters:1200\n",
            "Loss:0.9218680247328485\n",
            "Pred:[1 1 0 1 0 0 1 0]\n",
            "True:[0 1 1 1 1 0 1 0]\n",
            "11 + 111 = 210\n",
            "------------\n",
            "iters:1300\n",
            "Loss:1.0599478199948789\n",
            "Pred:[1 0 0 0 0 0 1 0]\n",
            "True:[1 0 1 0 0 1 1 1]\n",
            "42 + 125 = 130\n",
            "------------\n",
            "iters:1400\n",
            "Loss:0.8909178300488468\n",
            "Pred:[1 1 1 1 0 1 1 1]\n",
            "True:[0 1 1 0 0 1 0 1]\n",
            "101 + 0 = 247\n",
            "------------\n",
            "iters:1500\n",
            "Loss:1.0728624789741867\n",
            "Pred:[1 1 0 1 0 0 1 1]\n",
            "True:[0 1 0 0 1 1 0 0]\n",
            "64 + 12 = 211\n",
            "------------\n",
            "iters:1600\n",
            "Loss:0.9459094136148207\n",
            "Pred:[0 1 0 0 0 1 1 0]\n",
            "True:[0 1 0 0 0 0 1 0]\n",
            "7 + 59 = 70\n",
            "------------\n",
            "iters:1700\n",
            "Loss:0.943469628723946\n",
            "Pred:[0 1 0 1 0 0 0 1]\n",
            "True:[0 1 1 0 0 0 0 0]\n",
            "82 + 14 = 81\n",
            "------------\n",
            "iters:1800\n",
            "Loss:0.9417304735198859\n",
            "Pred:[0 1 1 1 0 0 1 0]\n",
            "True:[0 1 1 1 0 1 1 1]\n",
            "110 + 9 = 114\n",
            "------------\n",
            "iters:1900\n",
            "Loss:0.8440461355373389\n",
            "Pred:[0 0 1 0 1 1 1 1]\n",
            "True:[0 0 1 0 1 1 1 1]\n",
            "31 + 16 = 47\n",
            "------------\n",
            "iters:2000\n",
            "Loss:0.8945319644906334\n",
            "Pred:[0 0 0 1 0 1 1 0]\n",
            "True:[0 0 0 1 0 0 1 0]\n",
            "7 + 11 = 22\n",
            "------------\n",
            "iters:2100\n",
            "Loss:1.0040910503727425\n",
            "Pred:[0 0 0 0 1 0 1 0]\n",
            "True:[0 0 1 0 0 0 0 1]\n",
            "26 + 7 = 10\n",
            "------------\n",
            "iters:2200\n",
            "Loss:0.9575068011651603\n",
            "Pred:[0 0 0 0 0 0 0 1]\n",
            "True:[0 1 0 0 1 1 0 0]\n",
            "34 + 42 = 1\n",
            "------------\n",
            "iters:2300\n",
            "Loss:0.9911342512951506\n",
            "Pred:[1 0 1 0 0 0 0 1]\n",
            "True:[1 1 0 1 0 1 0 0]\n",
            "98 + 114 = 161\n",
            "------------\n",
            "iters:2400\n",
            "Loss:1.0233050951029101\n",
            "Pred:[0 1 0 0 1 0 1 0]\n",
            "True:[0 1 1 1 0 0 0 1]\n",
            "60 + 53 = 74\n",
            "------------\n",
            "iters:2500\n",
            "Loss:0.8748506109014438\n",
            "Pred:[0 0 1 0 0 0 0 1]\n",
            "True:[1 0 0 0 1 0 0 1]\n",
            "51 + 86 = 33\n",
            "------------\n",
            "iters:2600\n",
            "Loss:1.0226976299921127\n",
            "Pred:[0 1 1 0 1 0 0 0]\n",
            "True:[1 0 0 0 1 1 0 0]\n",
            "122 + 18 = 104\n",
            "------------\n",
            "iters:2700\n",
            "Loss:1.079623311719482\n",
            "Pred:[0 1 0 0 0 0 1 0]\n",
            "True:[1 1 0 0 1 1 0 1]\n",
            "86 + 119 = 66\n",
            "------------\n",
            "iters:2800\n",
            "Loss:1.017396793158519\n",
            "Pred:[1 1 0 0 0 1 0 1]\n",
            "True:[1 1 0 0 1 0 0 0]\n",
            "78 + 122 = 197\n",
            "------------\n",
            "iters:2900\n",
            "Loss:0.9991305223910271\n",
            "Pred:[1 1 1 1 1 0 1 0]\n",
            "True:[1 1 0 0 0 1 1 1]\n",
            "96 + 103 = 250\n",
            "------------\n",
            "iters:3000\n",
            "Loss:0.9777699640443933\n",
            "Pred:[1 1 1 1 1 0 1 0]\n",
            "True:[1 0 1 0 0 0 1 1]\n",
            "44 + 119 = 250\n",
            "------------\n",
            "iters:3100\n",
            "Loss:0.9958301861816601\n",
            "Pred:[0 0 0 1 0 0 1 0]\n",
            "True:[0 0 0 1 1 1 0 0]\n",
            "19 + 9 = 18\n",
            "------------\n",
            "iters:3200\n",
            "Loss:1.0111255652119744\n",
            "Pred:[0 0 1 0 0 0 1 0]\n",
            "True:[0 0 1 1 1 1 0 1]\n",
            "44 + 17 = 34\n",
            "------------\n",
            "iters:3300\n",
            "Loss:1.1479188039316746\n",
            "Pred:[1 1 1 1 1 0 1 0]\n",
            "True:[1 0 0 1 1 1 0 1]\n",
            "126 + 31 = 250\n",
            "------------\n",
            "iters:3400\n",
            "Loss:1.1310374292647152\n",
            "Pred:[0 1 1 1 1 0 1 1]\n",
            "True:[0 1 0 1 0 1 0 0]\n",
            "47 + 37 = 123\n",
            "------------\n",
            "iters:3500\n",
            "Loss:1.0720866569386338\n",
            "Pred:[1 0 1 1 0 0 1 1]\n",
            "True:[0 1 1 1 0 1 0 0]\n",
            "23 + 93 = 179\n",
            "------------\n",
            "iters:3600\n",
            "Loss:1.001477130059911\n",
            "Pred:[1 1 0 1 1 1 1 1]\n",
            "True:[1 0 1 0 0 1 1 0]\n",
            "70 + 96 = 223\n",
            "------------\n",
            "iters:3700\n",
            "Loss:0.977299582173788\n",
            "Pred:[0 1 1 0 1 1 1 1]\n",
            "True:[0 1 0 1 1 1 0 0]\n",
            "44 + 48 = 111\n",
            "------------\n",
            "iters:3800\n",
            "Loss:0.9304589429034109\n",
            "Pred:[1 0 0 1 0 0 1 0]\n",
            "True:[1 0 0 1 1 0 0 1]\n",
            "50 + 103 = 146\n",
            "------------\n",
            "iters:3900\n",
            "Loss:1.0326689039505639\n",
            "Pred:[0 0 0 1 0 0 1 0]\n",
            "True:[0 0 1 0 0 1 1 1]\n",
            "30 + 9 = 18\n",
            "------------\n",
            "iters:4000\n",
            "Loss:0.9690909120682224\n",
            "Pred:[0 0 0 1 0 1 0 0]\n",
            "True:[0 0 0 1 1 0 1 0]\n",
            "16 + 10 = 20\n",
            "------------\n",
            "iters:4100\n",
            "Loss:1.0656872986115418\n",
            "Pred:[0 0 0 0 1 1 0 1]\n",
            "True:[0 1 1 1 1 0 1 1]\n",
            "5 + 118 = 13\n",
            "------------\n",
            "iters:4200\n",
            "Loss:1.0375895868541567\n",
            "Pred:[0 0 0 0 0 1 0 1]\n",
            "True:[0 1 1 0 1 0 0 1]\n",
            "7 + 98 = 5\n",
            "------------\n",
            "iters:4300\n",
            "Loss:0.9522431279860423\n",
            "Pred:[1 1 1 1 1 0 1 1]\n",
            "True:[0 1 1 1 0 1 1 1]\n",
            "80 + 39 = 251\n",
            "------------\n",
            "iters:4400\n",
            "Loss:1.0535362768912362\n",
            "Pred:[1 1 0 1 0 1 0 1]\n",
            "True:[1 0 1 1 1 0 1 1]\n",
            "65 + 122 = 213\n",
            "------------\n",
            "iters:4500\n",
            "Loss:0.9823300342693932\n",
            "Pred:[0 0 1 1 1 1 0 1]\n",
            "True:[0 0 1 0 1 0 1 0]\n",
            "12 + 30 = 61\n",
            "------------\n",
            "iters:4600\n",
            "Loss:1.0167805745551273\n",
            "Pred:[0 0 1 0 1 0 0 1]\n",
            "True:[1 0 0 1 1 1 1 1]\n",
            "41 + 118 = 41\n",
            "------------\n",
            "iters:4700\n",
            "Loss:1.073201999845426\n",
            "Pred:[1 0 1 1 0 1 0 1]\n",
            "True:[0 1 0 1 1 0 1 0]\n",
            "0 + 90 = 181\n",
            "------------\n",
            "iters:4800\n",
            "Loss:1.0183215931138068\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 1 1 1]\n",
            "16 + 119 = 0\n",
            "------------\n",
            "iters:4900\n",
            "Loss:0.8986253481938589\n",
            "Pred:[0 0 0 0 0 0 1 0]\n",
            "True:[0 1 0 0 0 1 1 0]\n",
            "13 + 57 = 2\n",
            "------------\n",
            "iters:5000\n",
            "Loss:0.9184510295912427\n",
            "Pred:[0 0 1 1 0 0 1 0]\n",
            "True:[1 0 1 1 0 1 1 0]\n",
            "61 + 121 = 50\n",
            "------------\n",
            "iters:5100\n",
            "Loss:1.0041011765289298\n",
            "Pred:[0 1 1 1 1 0 1 1]\n",
            "True:[1 0 0 1 0 0 1 0]\n",
            "86 + 60 = 123\n",
            "------------\n",
            "iters:5200\n",
            "Loss:0.9018337802880786\n",
            "Pred:[0 0 1 0 1 0 0 0]\n",
            "True:[0 0 1 1 1 0 0 0]\n",
            "36 + 20 = 40\n",
            "------------\n",
            "iters:5300\n",
            "Loss:0.9842493521753485\n",
            "Pred:[0 1 0 1 1 0 0 0]\n",
            "True:[0 1 1 0 0 0 0 0]\n",
            "90 + 6 = 88\n",
            "------------\n",
            "iters:5400\n",
            "Loss:0.9964619985303662\n",
            "Pred:[0 0 1 0 1 0 0 0]\n",
            "True:[0 1 0 0 1 0 0 0]\n",
            "44 + 28 = 40\n",
            "------------\n",
            "iters:5500\n",
            "Loss:0.9817853385903956\n",
            "Pred:[0 1 0 1 1 1 0 0]\n",
            "True:[1 0 0 1 1 1 0 0]\n",
            "118 + 38 = 92\n",
            "------------\n",
            "iters:5600\n",
            "Loss:1.0059671899158529\n",
            "Pred:[1 0 1 1 1 0 1 1]\n",
            "True:[0 1 1 1 0 1 1 1]\n",
            "51 + 68 = 187\n",
            "------------\n",
            "iters:5700\n",
            "Loss:1.0100018022516104\n",
            "Pred:[0 0 1 0 1 0 1 0]\n",
            "True:[0 0 0 1 1 1 0 1]\n",
            "12 + 17 = 42\n",
            "------------\n",
            "iters:5800\n",
            "Loss:0.9102671036880108\n",
            "Pred:[1 0 1 1 1 1 1 1]\n",
            "True:[0 1 1 0 1 1 1 0]\n",
            "30 + 80 = 191\n",
            "------------\n",
            "iters:5900\n",
            "Loss:0.9272120974444299\n",
            "Pred:[1 0 1 1 1 0 1 1]\n",
            "True:[1 0 0 0 1 0 1 1]\n",
            "54 + 85 = 187\n",
            "------------\n",
            "iters:6000\n",
            "Loss:0.8848351626996507\n",
            "Pred:[0 0 0 0 0 0 1 0]\n",
            "True:[0 1 1 1 0 0 1 1]\n",
            "86 + 29 = 2\n",
            "------------\n",
            "iters:6100\n",
            "Loss:0.9727674503413275\n",
            "Pred:[0 1 0 0 1 1 0 0]\n",
            "True:[1 0 0 0 1 1 0 0]\n",
            "106 + 34 = 76\n",
            "------------\n",
            "iters:6200\n",
            "Loss:1.103435545625452\n",
            "Pred:[0 1 1 0 1 0 1 1]\n",
            "True:[1 0 1 0 1 1 0 0]\n",
            "123 + 49 = 107\n",
            "------------\n",
            "iters:6300\n",
            "Loss:0.9956995398191311\n",
            "Pred:[0 1 0 1 1 0 1 0]\n",
            "True:[1 0 0 0 1 0 1 0]\n",
            "94 + 44 = 90\n",
            "------------\n",
            "iters:6400\n",
            "Loss:1.0229119402355227\n",
            "Pred:[0 1 0 0 0 0 1 1]\n",
            "True:[1 0 0 0 1 0 0 0]\n",
            "81 + 55 = 67\n",
            "------------\n",
            "iters:6500\n",
            "Loss:0.979493308097902\n",
            "Pred:[0 1 0 1 0 0 0 0]\n",
            "True:[1 0 0 1 0 1 0 0]\n",
            "92 + 56 = 80\n",
            "------------\n",
            "iters:6600\n",
            "Loss:0.9311797372818973\n",
            "Pred:[1 0 0 0 1 1 0 0]\n",
            "True:[1 0 0 0 1 1 1 0]\n",
            "76 + 66 = 140\n",
            "------------\n",
            "iters:6700\n",
            "Loss:0.8854234717910334\n",
            "Pred:[0 0 0 1 0 1 0 1]\n",
            "True:[0 1 0 1 0 1 0 1]\n",
            "43 + 42 = 21\n",
            "------------\n",
            "iters:6800\n",
            "Loss:1.0377305715646017\n",
            "Pred:[1 0 1 1 0 0 1 1]\n",
            "True:[1 0 0 0 0 0 0 1]\n",
            "57 + 72 = 179\n",
            "------------\n",
            "iters:6900\n",
            "Loss:0.9178799688412383\n",
            "Pred:[1 0 0 1 0 1 1 1]\n",
            "True:[1 0 0 1 1 1 1 0]\n",
            "53 + 105 = 151\n",
            "------------\n",
            "iters:7000\n",
            "Loss:0.899734240828884\n",
            "Pred:[1 0 0 1 0 1 1 1]\n",
            "True:[1 0 0 1 0 1 1 0]\n",
            "46 + 104 = 151\n",
            "------------\n",
            "iters:7100\n",
            "Loss:0.8417154857110937\n",
            "Pred:[0 1 1 0 1 1 0 1]\n",
            "True:[0 1 1 1 1 1 0 1]\n",
            "103 + 22 = 109\n",
            "------------\n",
            "iters:7200\n",
            "Loss:0.9749823240660311\n",
            "Pred:[0 1 0 0 1 0 1 1]\n",
            "True:[0 0 1 1 1 0 1 0]\n",
            "9 + 49 = 75\n",
            "------------\n",
            "iters:7300\n",
            "Loss:1.0843060991145097\n",
            "Pred:[1 1 0 1 1 0 1 1]\n",
            "True:[1 1 0 1 0 1 0 0]\n",
            "111 + 101 = 219\n",
            "------------\n",
            "iters:7400\n",
            "Loss:0.9506986134205425\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 0 0 1 1 1]\n",
            "16 + 23 = 0\n",
            "------------\n",
            "iters:7500\n",
            "Loss:0.993291399872434\n",
            "Pred:[1 1 1 1 1 0 0 0]\n",
            "True:[1 1 1 1 0 1 1 0]\n",
            "120 + 126 = 248\n",
            "------------\n",
            "iters:7600\n",
            "Loss:1.0119071474066443\n",
            "Pred:[1 1 0 0 0 1 0 0]\n",
            "True:[1 0 1 1 1 1 0 0]\n",
            "68 + 120 = 196\n",
            "------------\n",
            "iters:7700\n",
            "Loss:0.9044365206888648\n",
            "Pred:[0 1 1 1 1 0 1 1]\n",
            "True:[0 1 1 0 1 0 0 1]\n",
            "52 + 53 = 123\n",
            "------------\n",
            "iters:7800\n",
            "Loss:0.9823590605929325\n",
            "Pred:[1 1 0 1 0 0 1 0]\n",
            "True:[1 0 1 1 1 0 1 0]\n",
            "82 + 104 = 210\n",
            "------------\n",
            "iters:7900\n",
            "Loss:0.8701281930461298\n",
            "Pred:[1 1 0 1 1 0 0 0]\n",
            "True:[1 1 0 1 1 1 0 0]\n",
            "124 + 96 = 216\n",
            "------------\n",
            "iters:8000\n",
            "Loss:0.9822707639127327\n",
            "Pred:[0 0 0 0 0 0 1 1]\n",
            "True:[0 1 1 0 0 0 0 1]\n",
            "2 + 95 = 3\n",
            "------------\n",
            "iters:8100\n",
            "Loss:0.9801489472279585\n",
            "Pred:[0 1 0 1 1 0 1 1]\n",
            "True:[0 1 1 0 1 0 1 1]\n",
            "92 + 15 = 91\n",
            "------------\n",
            "iters:8200\n",
            "Loss:1.005350311477028\n",
            "Pred:[1 1 0 1 1 0 1 1]\n",
            "True:[1 0 1 1 1 0 0 1]\n",
            "82 + 103 = 219\n",
            "------------\n",
            "iters:8300\n",
            "Loss:1.10643149393231\n",
            "Pred:[0 1 1 1 1 1 1 1]\n",
            "True:[1 0 0 1 0 0 0 1]\n",
            "109 + 36 = 127\n",
            "------------\n",
            "iters:8400\n",
            "Loss:0.9681826846545367\n",
            "Pred:[1 0 1 1 0 0 1 1]\n",
            "True:[0 1 1 1 0 0 1 0]\n",
            "35 + 79 = 179\n",
            "------------\n",
            "iters:8500\n",
            "Loss:0.929982480111549\n",
            "Pred:[1 1 0 0 1 0 1 0]\n",
            "True:[1 0 1 0 1 0 1 0]\n",
            "74 + 96 = 202\n",
            "------------\n",
            "iters:8600\n",
            "Loss:0.902682426316777\n",
            "Pred:[1 1 0 1 1 0 0 0]\n",
            "True:[1 1 0 0 1 0 0 0]\n",
            "90 + 110 = 216\n",
            "------------\n",
            "iters:8700\n",
            "Loss:0.942197518098094\n",
            "Pred:[0 1 1 1 1 0 1 1]\n",
            "True:[0 1 0 0 0 1 0 1]\n",
            "17 + 52 = 123\n",
            "------------\n",
            "iters:8800\n",
            "Loss:0.9232156075991459\n",
            "Pred:[0 1 0 1 1 0 1 1]\n",
            "True:[0 1 1 0 1 0 1 1]\n",
            "95 + 12 = 91\n",
            "------------\n",
            "iters:8900\n",
            "Loss:1.0333749723712111\n",
            "Pred:[0 0 1 0 1 0 1 1]\n",
            "True:[0 1 0 0 0 0 0 0]\n",
            "47 + 17 = 43\n",
            "------------\n",
            "iters:9000\n",
            "Loss:0.8412916279427892\n",
            "Pred:[1 0 0 1 0 0 1 1]\n",
            "True:[1 0 0 1 0 0 1 1]\n",
            "39 + 108 = 147\n",
            "------------\n",
            "iters:9100\n",
            "Loss:0.8441547470366897\n",
            "Pred:[1 0 1 0 1 0 1 1]\n",
            "True:[1 0 1 0 1 0 1 1]\n",
            "58 + 113 = 171\n",
            "------------\n",
            "iters:9200\n",
            "Loss:0.9752305040197453\n",
            "Pred:[0 0 1 0 0 1 0 0]\n",
            "True:[0 1 1 1 1 0 0 0]\n",
            "110 + 10 = 36\n",
            "------------\n",
            "iters:9300\n",
            "Loss:1.1620181022431844\n",
            "Pred:[1 0 0 1 0 1 1 1]\n",
            "True:[1 0 1 0 1 0 0 0]\n",
            "61 + 107 = 151\n",
            "------------\n",
            "iters:9400\n",
            "Loss:0.9241127883813083\n",
            "Pred:[1 0 1 1 0 0 1 1]\n",
            "True:[1 1 0 1 0 0 1 1]\n",
            "122 + 89 = 179\n",
            "------------\n",
            "iters:9500\n",
            "Loss:0.9781088775969009\n",
            "Pred:[0 0 0 0 0 1 1 1]\n",
            "True:[1 0 0 0 0 1 0 0]\n",
            "9 + 123 = 7\n",
            "------------\n",
            "iters:9600\n",
            "Loss:0.9012456060158403\n",
            "Pred:[0 0 1 0 0 0 0 0]\n",
            "True:[0 1 0 0 0 1 0 0]\n",
            "52 + 16 = 32\n",
            "------------\n",
            "iters:9700\n",
            "Loss:0.9576122652672711\n",
            "Pred:[1 1 1 0 0 1 0 0]\n",
            "True:[1 1 0 1 0 0 1 0]\n",
            "96 + 114 = 228\n",
            "------------\n",
            "iters:9800\n",
            "Loss:1.0238832948589538\n",
            "Pred:[0 1 0 0 0 1 1 1]\n",
            "True:[0 1 0 0 1 0 0 0]\n",
            "39 + 33 = 71\n",
            "------------\n",
            "iters:9900\n",
            "Loss:0.9953731456176558\n",
            "Pred:[1 0 0 1 0 0 1 0]\n",
            "True:[1 0 0 1 0 1 0 1]\n",
            "80 + 69 = 146\n",
            "------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bk9IUyZTYmMJ"
      },
      "source": [
        "lrを0.1から0.01にするとなかなか収束しない。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "guwfF8AXYp3_",
        "outputId": "7eafeb41-91e2-47d5-cd82-888edc4ad237"
      },
      "source": [
        "#[try] weight_init_stdやlearning_rate, hidden_layer_sizeを変更してみよう\n",
        "#lrを0.1のまま\n",
        "#hidden layerを32そうに増やしてみる\n",
        "\n",
        "import numpy as np\n",
        "from common import functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# def d_tanh(x):\n",
        "\n",
        "\n",
        "\n",
        "# データを用意\n",
        "# 2進数の桁数\n",
        "binary_dim = 8\n",
        "# 最大値 + 1\n",
        "largest_number = pow(2, binary_dim)\n",
        "# largest_numberまで2進数を用意\n",
        "binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
        "\n",
        "input_layer_size = 2\n",
        "hidden_layer_size = 32\n",
        "output_layer_size = 1\n",
        "\n",
        "weight_init_std = 1\n",
        "learning_rate = 0.1\n",
        "\n",
        "iters_num = 10000\n",
        "plot_interval = 100\n",
        "\n",
        "# ウェイト初期化 (バイアスは簡単のため省略)\n",
        "W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n",
        "W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n",
        "W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n",
        "\n",
        "# Xavier\n",
        "\n",
        "\n",
        "# He\n",
        "\n",
        "\n",
        "\n",
        "# 勾配\n",
        "W_in_grad = np.zeros_like(W_in)\n",
        "W_out_grad = np.zeros_like(W_out)\n",
        "W_grad = np.zeros_like(W)\n",
        "\n",
        "u = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "z = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "y = np.zeros((output_layer_size, binary_dim))\n",
        "\n",
        "delta_out = np.zeros((output_layer_size, binary_dim))\n",
        "delta = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "\n",
        "all_losses = []\n",
        "\n",
        "for i in range(iters_num):\n",
        "    \n",
        "    # A, B初期化 (a + b = d)\n",
        "    a_int = np.random.randint(largest_number/2)\n",
        "    a_bin = binary[a_int] # binary encoding\n",
        "    b_int = np.random.randint(largest_number/2)\n",
        "    b_bin = binary[b_int] # binary encoding\n",
        "    \n",
        "    # 正解データ\n",
        "    d_int = a_int + b_int\n",
        "    d_bin = binary[d_int]\n",
        "    \n",
        "    # 出力バイナリ\n",
        "    out_bin = np.zeros_like(d_bin)\n",
        "    \n",
        "    # 時系列全体の誤差\n",
        "    all_loss = 0    \n",
        "    \n",
        "    # 時系列ループ\n",
        "    for t in range(binary_dim):\n",
        "        # 入力値\n",
        "        X = np.array([a_bin[ - t - 1], b_bin[ - t - 1]]).reshape(1, -1)\n",
        "        # 時刻tにおける正解データ\n",
        "        dd = np.array([d_bin[binary_dim - t - 1]])\n",
        "        \n",
        "        u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n",
        "        z[:,t+1] = functions.sigmoid(u[:,t+1])\n",
        "\n",
        "        y[:,t] = functions.sigmoid(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
        "\n",
        "\n",
        "        #誤差\n",
        "        loss = functions.mean_squared_error(dd, y[:,t])\n",
        "        \n",
        "        delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * functions.d_sigmoid(y[:,t])        \n",
        "        \n",
        "        all_loss += loss\n",
        "\n",
        "        out_bin[binary_dim - t - 1] = np.round(y[:,t])\n",
        "    \n",
        "    \n",
        "    for t in range(binary_dim)[::-1]:\n",
        "        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n",
        "\n",
        "        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_sigmoid(u[:,t+1])\n",
        "\n",
        "        # 勾配更新\n",
        "        W_out_grad += np.dot(z[:,t+1].reshape(-1,1), delta_out[:,t].reshape(-1,1))\n",
        "        W_grad += np.dot(z[:,t].reshape(-1,1), delta[:,t].reshape(1,-1))\n",
        "        W_in_grad += np.dot(X.T, delta[:,t].reshape(1,-1))\n",
        "    \n",
        "    # 勾配適用\n",
        "    W_in -= learning_rate * W_in_grad\n",
        "    W_out -= learning_rate * W_out_grad\n",
        "    W -= learning_rate * W_grad\n",
        "    \n",
        "    W_in_grad *= 0\n",
        "    W_out_grad *= 0\n",
        "    W_grad *= 0\n",
        "    \n",
        "\n",
        "    if(i % plot_interval == 0):\n",
        "        all_losses.append(all_loss)        \n",
        "        print(\"iters:\" + str(i))\n",
        "        print(\"Loss:\" + str(all_loss))\n",
        "        print(\"Pred:\" + str(out_bin))\n",
        "        print(\"True:\" + str(d_bin))\n",
        "        out_int = 0\n",
        "        for index,x in enumerate(reversed(out_bin)):\n",
        "            out_int += x * pow(2, index)\n",
        "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out_int))\n",
        "        print(\"------------\")\n",
        "\n",
        "lists = range(0, iters_num, plot_interval)\n",
        "plt.plot(lists, all_losses, label=\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iters:0\n",
            "Loss:2.4951161139119065\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[1 0 1 0 0 0 0 1]\n",
            "50 + 111 = 255\n",
            "------------\n",
            "iters:100\n",
            "Loss:0.9271615521698214\n",
            "Pred:[1 0 1 0 1 1 1 1]\n",
            "True:[1 0 0 0 1 0 0 1]\n",
            "113 + 24 = 175\n",
            "------------\n",
            "iters:200\n",
            "Loss:0.9396891139071013\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[1 1 0 0 1 1 1 0]\n",
            "115 + 91 = 255\n",
            "------------\n",
            "iters:300\n",
            "Loss:1.05645308901339\n",
            "Pred:[0 0 0 0 1 0 1 1]\n",
            "True:[0 0 1 0 0 1 1 1]\n",
            "3 + 36 = 11\n",
            "------------\n",
            "iters:400\n",
            "Loss:1.1272839459404587\n",
            "Pred:[0 0 1 1 0 1 0 1]\n",
            "True:[1 0 0 1 1 0 0 0]\n",
            "78 + 74 = 53\n",
            "------------\n",
            "iters:500\n",
            "Loss:0.9788179830523911\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[1 1 0 1 0 0 1 1]\n",
            "123 + 88 = 255\n",
            "------------\n",
            "iters:600\n",
            "Loss:1.1893846509490948\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 0 0 1 0]\n",
            "16 + 98 = 0\n",
            "------------\n",
            "iters:700\n",
            "Loss:1.140457640342532\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[0 0 1 1 0 1 1 0]\n",
            "21 + 33 = 255\n",
            "------------\n",
            "iters:800\n",
            "Loss:0.7493929356887741\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[1 0 1 1 1 1 0 1]\n",
            "76 + 113 = 255\n",
            "------------\n",
            "iters:900\n",
            "Loss:0.9902497261690628\n",
            "Pred:[1 1 1 1 1 0 0 1]\n",
            "True:[0 1 0 1 1 0 0 0]\n",
            "68 + 20 = 249\n",
            "------------\n",
            "iters:1000\n",
            "Loss:0.9521148718913147\n",
            "Pred:[0 0 0 0 1 0 1 1]\n",
            "True:[0 0 0 1 0 1 1 1]\n",
            "3 + 20 = 11\n",
            "------------\n",
            "iters:1100\n",
            "Loss:0.9052255038222158\n",
            "Pred:[1 1 1 0 1 1 1 1]\n",
            "True:[0 1 1 0 0 1 0 0]\n",
            "82 + 18 = 239\n",
            "------------\n",
            "iters:1200\n",
            "Loss:0.9042040108090365\n",
            "Pred:[0 1 1 0 0 0 1 0]\n",
            "True:[0 1 1 1 0 0 0 0]\n",
            "53 + 59 = 98\n",
            "------------\n",
            "iters:1300\n",
            "Loss:1.1039693372852935\n",
            "Pred:[1 1 1 1 1 0 0 1]\n",
            "True:[0 1 1 0 0 0 0 0]\n",
            "36 + 60 = 249\n",
            "------------\n",
            "iters:1400\n",
            "Loss:0.7903181019777645\n",
            "Pred:[0 0 0 0 0 0 1 0]\n",
            "True:[0 1 0 0 0 1 1 0]\n",
            "65 + 5 = 2\n",
            "------------\n",
            "iters:1500\n",
            "Loss:0.7557977852740695\n",
            "Pred:[1 0 0 0 0 1 1 0]\n",
            "True:[1 0 1 1 0 1 1 0]\n",
            "81 + 101 = 134\n",
            "------------\n",
            "iters:1600\n",
            "Loss:0.9002155919942089\n",
            "Pred:[1 1 0 0 0 0 1 1]\n",
            "True:[1 1 0 1 1 1 1 0]\n",
            "106 + 116 = 195\n",
            "------------\n",
            "iters:1700\n",
            "Loss:0.6106667483497741\n",
            "Pred:[0 0 0 0 0 0 1 1]\n",
            "True:[0 1 0 0 0 0 1 1]\n",
            "64 + 3 = 3\n",
            "------------\n",
            "iters:1800\n",
            "Loss:0.9861846783463365\n",
            "Pred:[1 0 1 0 0 0 0 0]\n",
            "True:[1 1 0 0 0 1 1 1]\n",
            "117 + 82 = 160\n",
            "------------\n",
            "iters:1900\n",
            "Loss:0.7446979589239945\n",
            "Pred:[1 1 0 1 1 0 1 1]\n",
            "True:[1 0 0 1 1 0 1 1]\n",
            "54 + 101 = 219\n",
            "------------\n",
            "iters:2000\n",
            "Loss:1.2472605164576407\n",
            "Pred:[1 1 0 1 1 1 1 1]\n",
            "True:[1 0 1 0 0 0 0 1]\n",
            "99 + 62 = 223\n",
            "------------\n",
            "iters:2100\n",
            "Loss:1.0491608089787872\n",
            "Pred:[1 0 0 0 1 0 1 0]\n",
            "True:[1 0 1 0 0 0 1 1]\n",
            "101 + 62 = 138\n",
            "------------\n",
            "iters:2200\n",
            "Loss:0.8880678122105392\n",
            "Pred:[1 0 1 1 1 1 0 1]\n",
            "True:[1 0 1 1 0 0 0 1]\n",
            "87 + 90 = 189\n",
            "------------\n",
            "iters:2300\n",
            "Loss:0.5671256365643469\n",
            "Pred:[0 1 1 0 1 0 0 0]\n",
            "True:[0 1 1 0 1 0 0 1]\n",
            "52 + 53 = 104\n",
            "------------\n",
            "iters:2400\n",
            "Loss:0.7734869811675934\n",
            "Pred:[0 1 1 1 0 0 1 0]\n",
            "True:[0 1 0 0 0 0 1 0]\n",
            "52 + 14 = 114\n",
            "------------\n",
            "iters:2500\n",
            "Loss:0.38045755391637576\n",
            "Pred:[1 0 1 1 1 0 0 1]\n",
            "True:[1 0 1 1 1 0 0 0]\n",
            "108 + 76 = 185\n",
            "------------\n",
            "iters:2600\n",
            "Loss:0.2562757605676117\n",
            "Pred:[0 0 0 1 0 1 1 0]\n",
            "True:[0 0 0 1 0 1 1 0]\n",
            "18 + 4 = 22\n",
            "------------\n",
            "iters:2700\n",
            "Loss:0.8747068730977932\n",
            "Pred:[1 1 0 1 1 1 0 0]\n",
            "True:[1 0 0 0 0 0 0 0]\n",
            "113 + 15 = 220\n",
            "------------\n",
            "iters:2800\n",
            "Loss:0.5535531640209588\n",
            "Pred:[0 1 1 1 1 1 1 0]\n",
            "True:[0 1 1 1 0 0 1 0]\n",
            "87 + 27 = 126\n",
            "------------\n",
            "iters:2900\n",
            "Loss:0.7094888550805147\n",
            "Pred:[0 0 0 0 1 1 1 0]\n",
            "True:[0 0 1 1 1 1 1 0]\n",
            "62 + 0 = 14\n",
            "------------\n",
            "iters:3000\n",
            "Loss:0.27631256712716545\n",
            "Pred:[1 0 0 0 1 0 1 1]\n",
            "True:[1 0 0 0 1 0 1 1]\n",
            "23 + 116 = 139\n",
            "------------\n",
            "iters:3100\n",
            "Loss:0.35466589556344336\n",
            "Pred:[1 0 0 1 0 0 0 0]\n",
            "True:[1 0 0 1 0 1 0 0]\n",
            "109 + 39 = 144\n",
            "------------\n",
            "iters:3200\n",
            "Loss:1.4407444635071553\n",
            "Pred:[1 0 0 1 1 0 1 0]\n",
            "True:[1 0 1 0 0 0 1 0]\n",
            "85 + 77 = 154\n",
            "------------\n",
            "iters:3300\n",
            "Loss:0.2557702323409556\n",
            "Pred:[1 1 0 0 0 1 1 1]\n",
            "True:[1 1 0 0 0 1 1 1]\n",
            "89 + 110 = 199\n",
            "------------\n",
            "iters:3400\n",
            "Loss:0.075921279633536\n",
            "Pred:[1 0 1 1 0 1 1 0]\n",
            "True:[1 0 1 1 0 1 1 0]\n",
            "90 + 92 = 182\n",
            "------------\n",
            "iters:3500\n",
            "Loss:0.07455861038966437\n",
            "Pred:[1 1 0 0 0 0 1 0]\n",
            "True:[1 1 0 0 0 0 1 0]\n",
            "101 + 93 = 194\n",
            "------------\n",
            "iters:3600\n",
            "Loss:0.3334391813392628\n",
            "Pred:[0 0 1 0 1 1 1 1]\n",
            "True:[0 0 1 0 1 1 1 1]\n",
            "47 + 0 = 47\n",
            "------------\n",
            "iters:3700\n",
            "Loss:0.07431698911929488\n",
            "Pred:[1 0 0 1 0 1 1 0]\n",
            "True:[1 0 0 1 0 1 1 0]\n",
            "122 + 28 = 150\n",
            "------------\n",
            "iters:3800\n",
            "Loss:0.07452934840437252\n",
            "Pred:[1 0 1 1 0 1 1 0]\n",
            "True:[1 0 1 1 0 1 1 0]\n",
            "90 + 92 = 182\n",
            "------------\n",
            "iters:3900\n",
            "Loss:0.11385298199469632\n",
            "Pred:[1 0 1 1 0 1 0 0]\n",
            "True:[1 0 1 1 0 1 0 0]\n",
            "63 + 117 = 180\n",
            "------------\n",
            "iters:4000\n",
            "Loss:0.06322020783835361\n",
            "Pred:[1 0 1 0 1 1 1 0]\n",
            "True:[1 0 1 0 1 1 1 0]\n",
            "78 + 96 = 174\n",
            "------------\n",
            "iters:4100\n",
            "Loss:0.0501372932664109\n",
            "Pred:[1 1 1 0 1 0 1 0]\n",
            "True:[1 1 1 0 1 0 1 0]\n",
            "118 + 116 = 234\n",
            "------------\n",
            "iters:4200\n",
            "Loss:0.04735770715289783\n",
            "Pred:[1 0 0 1 1 0 1 0]\n",
            "True:[1 0 0 1 1 0 1 0]\n",
            "30 + 124 = 154\n",
            "------------\n",
            "iters:4300\n",
            "Loss:0.09462571228223486\n",
            "Pred:[1 1 0 0 1 0 0 0]\n",
            "True:[1 1 0 0 1 0 0 0]\n",
            "118 + 82 = 200\n",
            "------------\n",
            "iters:4400\n",
            "Loss:0.024018282165373182\n",
            "Pred:[0 1 1 0 1 0 0 1]\n",
            "True:[0 1 1 0 1 0 0 1]\n",
            "71 + 34 = 105\n",
            "------------\n",
            "iters:4500\n",
            "Loss:0.011519041570337772\n",
            "Pred:[1 0 1 0 1 0 1 0]\n",
            "True:[1 0 1 0 1 0 1 0]\n",
            "97 + 73 = 170\n",
            "------------\n",
            "iters:4600\n",
            "Loss:0.11566810641105109\n",
            "Pred:[1 0 0 1 0 1 1 1]\n",
            "True:[1 0 0 1 0 1 1 1]\n",
            "73 + 78 = 151\n",
            "------------\n",
            "iters:4700\n",
            "Loss:0.11328317694779924\n",
            "Pred:[0 1 1 1 1 1 0 1]\n",
            "True:[0 1 1 1 1 1 0 1]\n",
            "18 + 107 = 125\n",
            "------------\n",
            "iters:4800\n",
            "Loss:0.89096050567207\n",
            "Pred:[0 1 0 1 0 1 1 0]\n",
            "True:[1 0 0 1 0 1 1 0]\n",
            "120 + 30 = 86\n",
            "------------\n",
            "iters:4900\n",
            "Loss:0.025703461770668286\n",
            "Pred:[0 1 0 1 1 0 0 1]\n",
            "True:[0 1 0 1 1 0 0 1]\n",
            "87 + 2 = 89\n",
            "------------\n",
            "iters:5000\n",
            "Loss:0.08377008396950371\n",
            "Pred:[1 0 0 0 0 1 0 1]\n",
            "True:[1 0 0 0 0 1 0 1]\n",
            "86 + 47 = 133\n",
            "------------\n",
            "iters:5100\n",
            "Loss:0.016969144926045062\n",
            "Pred:[1 0 1 1 0 0 1 1]\n",
            "True:[1 0 1 1 0 0 1 1]\n",
            "82 + 97 = 179\n",
            "------------\n",
            "iters:5200\n",
            "Loss:0.01615868455688547\n",
            "Pred:[0 1 1 0 0 1 1 1]\n",
            "True:[0 1 1 0 0 1 1 1]\n",
            "90 + 13 = 103\n",
            "------------\n",
            "iters:5300\n",
            "Loss:0.03919928206189848\n",
            "Pred:[1 0 0 1 0 0 0 0]\n",
            "True:[1 0 0 1 0 0 0 0]\n",
            "98 + 46 = 144\n",
            "------------\n",
            "iters:5400\n",
            "Loss:0.015380786601545205\n",
            "Pred:[1 1 0 0 0 1 0 1]\n",
            "True:[1 1 0 0 0 1 0 1]\n",
            "100 + 97 = 197\n",
            "------------\n",
            "iters:5500\n",
            "Loss:0.012574430725620703\n",
            "Pred:[1 0 1 1 0 0 0 0]\n",
            "True:[1 0 1 1 0 0 0 0]\n",
            "64 + 112 = 176\n",
            "------------\n",
            "iters:5600\n",
            "Loss:0.0058213901946488865\n",
            "Pred:[0 1 1 1 1 0 0 1]\n",
            "True:[0 1 1 1 1 0 0 1]\n",
            "1 + 120 = 121\n",
            "------------\n",
            "iters:5700\n",
            "Loss:0.00526315766993864\n",
            "Pred:[1 0 1 1 0 0 1 1]\n",
            "True:[1 0 1 1 0 0 1 1]\n",
            "59 + 120 = 179\n",
            "------------\n",
            "iters:5800\n",
            "Loss:0.0057704608124208795\n",
            "Pred:[0 1 0 1 1 0 1 1]\n",
            "True:[0 1 0 1 1 0 1 1]\n",
            "25 + 66 = 91\n",
            "------------\n",
            "iters:5900\n",
            "Loss:0.012420905320804788\n",
            "Pred:[1 1 0 0 1 0 1 1]\n",
            "True:[1 1 0 0 1 0 1 1]\n",
            "80 + 123 = 203\n",
            "------------\n",
            "iters:6000\n",
            "Loss:0.011342139974043689\n",
            "Pred:[1 0 0 0 1 0 0 1]\n",
            "True:[1 0 0 0 1 0 0 1]\n",
            "29 + 108 = 137\n",
            "------------\n",
            "iters:6100\n",
            "Loss:0.005428921675146075\n",
            "Pred:[0 0 0 1 1 0 1 1]\n",
            "True:[0 0 0 1 1 0 1 1]\n",
            "23 + 4 = 27\n",
            "------------\n",
            "iters:6200\n",
            "Loss:0.007564074545092162\n",
            "Pred:[1 0 0 0 0 0 1 1]\n",
            "True:[1 0 0 0 0 0 1 1]\n",
            "62 + 69 = 131\n",
            "------------\n",
            "iters:6300\n",
            "Loss:0.005548040218083626\n",
            "Pred:[1 1 0 1 1 1 0 1]\n",
            "True:[1 1 0 1 1 1 0 1]\n",
            "121 + 100 = 221\n",
            "------------\n",
            "iters:6400\n",
            "Loss:0.001775416471385001\n",
            "Pred:[1 0 0 0 0 1 1 0]\n",
            "True:[1 0 0 0 0 1 1 0]\n",
            "25 + 109 = 134\n",
            "------------\n",
            "iters:6500\n",
            "Loss:0.0016935568438102328\n",
            "Pred:[0 0 0 1 1 1 0 0]\n",
            "True:[0 0 0 1 1 1 0 0]\n",
            "23 + 5 = 28\n",
            "------------\n",
            "iters:6600\n",
            "Loss:0.0034637977233355184\n",
            "Pred:[0 1 1 1 1 1 0 1]\n",
            "True:[0 1 1 1 1 1 0 1]\n",
            "120 + 5 = 125\n",
            "------------\n",
            "iters:6700\n",
            "Loss:0.0044849667109179425\n",
            "Pred:[0 0 1 1 0 1 1 1]\n",
            "True:[0 0 1 1 0 1 1 1]\n",
            "6 + 49 = 55\n",
            "------------\n",
            "iters:6800\n",
            "Loss:0.0018242543471579048\n",
            "Pred:[0 1 0 0 0 1 0 0]\n",
            "True:[0 1 0 0 0 1 0 0]\n",
            "57 + 11 = 68\n",
            "------------\n",
            "iters:6900\n",
            "Loss:0.002611887635250258\n",
            "Pred:[1 0 1 1 1 1 1 1]\n",
            "True:[1 0 1 1 1 1 1 1]\n",
            "95 + 96 = 191\n",
            "------------\n",
            "iters:7000\n",
            "Loss:0.002426241659783615\n",
            "Pred:[1 0 1 0 1 1 0 1]\n",
            "True:[1 0 1 0 1 1 0 1]\n",
            "53 + 120 = 173\n",
            "------------\n",
            "iters:7100\n",
            "Loss:0.002952297271041096\n",
            "Pred:[0 1 0 0 1 0 1 0]\n",
            "True:[0 1 0 0 1 0 1 0]\n",
            "16 + 58 = 74\n",
            "------------\n",
            "iters:7200\n",
            "Loss:0.0018902074373402626\n",
            "Pred:[0 1 1 0 0 1 0 1]\n",
            "True:[0 1 1 0 0 1 0 1]\n",
            "45 + 56 = 101\n",
            "------------\n",
            "iters:7300\n",
            "Loss:0.0026080303923904256\n",
            "Pred:[1 0 0 1 1 1 1 0]\n",
            "True:[1 0 0 1 1 1 1 0]\n",
            "54 + 104 = 158\n",
            "------------\n",
            "iters:7400\n",
            "Loss:0.0025112563127923505\n",
            "Pred:[0 0 1 0 0 0 0 0]\n",
            "True:[0 0 1 0 0 0 0 0]\n",
            "1 + 31 = 32\n",
            "------------\n",
            "iters:7500\n",
            "Loss:0.0025628025485862796\n",
            "Pred:[1 0 0 1 0 0 0 1]\n",
            "True:[1 0 0 1 0 0 0 1]\n",
            "62 + 83 = 145\n",
            "------------\n",
            "iters:7600\n",
            "Loss:0.0020006028842466106\n",
            "Pred:[0 1 0 1 0 1 0 1]\n",
            "True:[0 1 0 1 0 1 0 1]\n",
            "4 + 81 = 85\n",
            "------------\n",
            "iters:7700\n",
            "Loss:0.0014211318805629415\n",
            "Pred:[1 0 1 1 0 0 0 0]\n",
            "True:[1 0 1 1 0 0 0 0]\n",
            "73 + 103 = 176\n",
            "------------\n",
            "iters:7800\n",
            "Loss:0.0013584840676986588\n",
            "Pred:[1 1 0 0 0 0 1 1]\n",
            "True:[1 1 0 0 0 0 1 1]\n",
            "103 + 92 = 195\n",
            "------------\n",
            "iters:7900\n",
            "Loss:0.0017958030336903954\n",
            "Pred:[0 0 1 0 1 1 0 1]\n",
            "True:[0 0 1 0 1 1 0 1]\n",
            "27 + 18 = 45\n",
            "------------\n",
            "iters:8000\n",
            "Loss:0.0012736245800585902\n",
            "Pred:[1 0 0 0 0 0 0 1]\n",
            "True:[1 0 0 0 0 0 0 1]\n",
            "97 + 32 = 129\n",
            "------------\n",
            "iters:8100\n",
            "Loss:0.0019275210490981986\n",
            "Pred:[0 1 1 1 1 0 1 0]\n",
            "True:[0 1 1 1 1 0 1 0]\n",
            "64 + 58 = 122\n",
            "------------\n",
            "iters:8200\n",
            "Loss:0.001367714812718183\n",
            "Pred:[0 1 0 1 1 1 1 1]\n",
            "True:[0 1 0 1 1 1 1 1]\n",
            "55 + 40 = 95\n",
            "------------\n",
            "iters:8300\n",
            "Loss:0.0005172618809419264\n",
            "Pred:[1 0 0 0 1 0 1 0]\n",
            "True:[1 0 0 0 1 0 1 0]\n",
            "87 + 51 = 138\n",
            "------------\n",
            "iters:8400\n",
            "Loss:0.0020462175612684253\n",
            "Pred:[1 0 1 1 1 1 1 1]\n",
            "True:[1 0 1 1 1 1 1 1]\n",
            "118 + 73 = 191\n",
            "------------\n",
            "iters:8500\n",
            "Loss:0.0015507103049629111\n",
            "Pred:[1 0 0 1 1 0 0 0]\n",
            "True:[1 0 0 1 1 0 0 0]\n",
            "64 + 88 = 152\n",
            "------------\n",
            "iters:8600\n",
            "Loss:0.0015000782582690894\n",
            "Pred:[1 1 1 0 0 1 1 0]\n",
            "True:[1 1 1 0 0 1 1 0]\n",
            "106 + 124 = 230\n",
            "------------\n",
            "iters:8700\n",
            "Loss:0.0005538855750006284\n",
            "Pred:[1 0 0 1 0 1 1 0]\n",
            "True:[1 0 0 1 0 1 1 0]\n",
            "27 + 123 = 150\n",
            "------------\n",
            "iters:8800\n",
            "Loss:0.0024690708078888715\n",
            "Pred:[1 0 1 1 1 0 0 0]\n",
            "True:[1 0 1 1 1 0 0 0]\n",
            "116 + 68 = 184\n",
            "------------\n",
            "iters:8900\n",
            "Loss:0.0013087074700618643\n",
            "Pred:[0 0 0 1 0 0 0 0]\n",
            "True:[0 0 0 1 0 0 0 0]\n",
            "0 + 16 = 16\n",
            "------------\n",
            "iters:9000\n",
            "Loss:0.00047428832385646975\n",
            "Pred:[1 0 0 0 1 0 1 0]\n",
            "True:[1 0 0 0 1 0 1 0]\n",
            "57 + 81 = 138\n",
            "------------\n",
            "iters:9100\n",
            "Loss:0.0010996386505841146\n",
            "Pred:[1 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 0 0]\n",
            "97 + 31 = 128\n",
            "------------\n",
            "iters:9200\n",
            "Loss:0.0011006139157216923\n",
            "Pred:[1 0 0 0 1 1 0 1]\n",
            "True:[1 0 0 0 1 1 0 1]\n",
            "40 + 101 = 141\n",
            "------------\n",
            "iters:9300\n",
            "Loss:0.0011990275762423408\n",
            "Pred:[0 1 1 1 1 0 1 0]\n",
            "True:[0 1 1 1 1 0 1 0]\n",
            "118 + 4 = 122\n",
            "------------\n",
            "iters:9400\n",
            "Loss:0.0008569497030270559\n",
            "Pred:[1 0 0 0 0 0 0 1]\n",
            "True:[1 0 0 0 0 0 0 1]\n",
            "51 + 78 = 129\n",
            "------------\n",
            "iters:9500\n",
            "Loss:0.0018106226904564644\n",
            "Pred:[0 1 1 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 0 0 0]\n",
            "6 + 90 = 96\n",
            "------------\n",
            "iters:9600\n",
            "Loss:0.0011200398399901642\n",
            "Pred:[1 0 0 0 1 1 1 0]\n",
            "True:[1 0 0 0 1 1 1 0]\n",
            "96 + 46 = 142\n",
            "------------\n",
            "iters:9700\n",
            "Loss:0.0013059708313966172\n",
            "Pred:[1 0 0 1 0 0 1 0]\n",
            "True:[1 0 0 1 0 0 1 0]\n",
            "20 + 126 = 146\n",
            "------------\n",
            "iters:9800\n",
            "Loss:0.0006703884368795502\n",
            "Pred:[1 0 0 0 0 1 1 1]\n",
            "True:[1 0 0 0 0 1 1 1]\n",
            "95 + 40 = 135\n",
            "------------\n",
            "iters:9900\n",
            "Loss:0.0009391796152899513\n",
            "Pred:[1 1 0 0 1 0 1 1]\n",
            "True:[1 1 0 0 1 0 1 1]\n",
            "106 + 97 = 203\n",
            "------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OC3aRvSSY3r-"
      },
      "source": [
        "層を増やしても計算コストはそこまで上がらず。精度は6000回程度で落ち着いている。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKUh3C6VZpAu"
      },
      "source": [
        "#中間層をReLUにして勾配爆発を確認する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OL7fi_unZkc0",
        "outputId": "a46f6628-b01a-4e76-f541-8da15eaea2ab"
      },
      "source": [
        "#[try] 中間層の活性化関数を変更してみよう\n",
        "#ReLU(勾配爆発を確認しよう)\n",
        "#tanh(numpyにtanhが用意されている。導関数をd_tanhとして作成しよう)\n",
        "\n",
        "import numpy as np\n",
        "from common import functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# def d_tanh(x):\n",
        "\n",
        "\n",
        "\n",
        "# データを用意\n",
        "# 2進数の桁数\n",
        "binary_dim = 8\n",
        "# 最大値 + 1\n",
        "largest_number = pow(2, binary_dim)\n",
        "# largest_numberまで2進数を用意\n",
        "binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
        "\n",
        "input_layer_size = 2\n",
        "hidden_layer_size = 32\n",
        "output_layer_size = 1\n",
        "\n",
        "weight_init_std = 1\n",
        "learning_rate = 0.1\n",
        "\n",
        "iters_num = 10000\n",
        "plot_interval = 100\n",
        "\n",
        "# ウェイト初期化 (バイアスは簡単のため省略)\n",
        "W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n",
        "W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n",
        "W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n",
        "\n",
        "# Xavier\n",
        "\n",
        "\n",
        "# He\n",
        "\n",
        "\n",
        "\n",
        "# 勾配\n",
        "W_in_grad = np.zeros_like(W_in)\n",
        "W_out_grad = np.zeros_like(W_out)\n",
        "W_grad = np.zeros_like(W)\n",
        "\n",
        "u = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "z = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "y = np.zeros((output_layer_size, binary_dim))\n",
        "\n",
        "delta_out = np.zeros((output_layer_size, binary_dim))\n",
        "delta = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "\n",
        "all_losses = []\n",
        "\n",
        "for i in range(iters_num):\n",
        "    \n",
        "    # A, B初期化 (a + b = d)\n",
        "    a_int = np.random.randint(largest_number/2)\n",
        "    a_bin = binary[a_int] # binary encoding\n",
        "    b_int = np.random.randint(largest_number/2)\n",
        "    b_bin = binary[b_int] # binary encoding\n",
        "    \n",
        "    # 正解データ\n",
        "    d_int = a_int + b_int\n",
        "    d_bin = binary[d_int]\n",
        "    \n",
        "    # 出力バイナリ\n",
        "    out_bin = np.zeros_like(d_bin)\n",
        "    \n",
        "    # 時系列全体の誤差\n",
        "    all_loss = 0    \n",
        "    \n",
        "    # 時系列ループ\n",
        "    for t in range(binary_dim):\n",
        "        # 入力値\n",
        "        X = np.array([a_bin[ - t - 1], b_bin[ - t - 1]]).reshape(1, -1)\n",
        "        # 時刻tにおける正解データ\n",
        "        dd = np.array([d_bin[binary_dim - t - 1]])\n",
        "        \n",
        "        u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n",
        "        z[:,t+1] = functions.relu(u[:,t+1])\n",
        "\n",
        "        y[:,t] = functions.relu(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
        "\n",
        "\n",
        "        #誤差\n",
        "        loss = functions.mean_squared_error(dd, y[:,t])\n",
        "        \n",
        "        delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * functions.d_relu(y[:,t])        \n",
        "        \n",
        "        all_loss += loss\n",
        "\n",
        "        out_bin[binary_dim - t - 1] = np.round(y[:,t])\n",
        "    \n",
        "    \n",
        "    for t in range(binary_dim)[::-1]:\n",
        "        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n",
        "\n",
        "        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_relu(u[:,t+1])\n",
        "\n",
        "        # 勾配更新\n",
        "        W_out_grad += np.dot(z[:,t+1].reshape(-1,1), delta_out[:,t].reshape(-1,1))\n",
        "        W_grad += np.dot(z[:,t].reshape(-1,1), delta[:,t].reshape(1,-1))\n",
        "        W_in_grad += np.dot(X.T, delta[:,t].reshape(1,-1))\n",
        "    \n",
        "    # 勾配適用\n",
        "    W_in -= learning_rate * W_in_grad\n",
        "    W_out -= learning_rate * W_out_grad\n",
        "    W -= learning_rate * W_grad\n",
        "    \n",
        "    W_in_grad *= 0\n",
        "    W_out_grad *= 0\n",
        "    W_grad *= 0\n",
        "    \n",
        "\n",
        "    if(i % plot_interval == 0):\n",
        "        all_losses.append(all_loss)        \n",
        "        print(\"iters:\" + str(i))\n",
        "        print(\"Loss:\" + str(all_loss))\n",
        "        print(\"Pred:\" + str(out_bin))\n",
        "        print(\"True:\" + str(d_bin))\n",
        "        out_int = 0\n",
        "        for index,x in enumerate(reversed(out_bin)):\n",
        "            out_int += x * pow(2, index)\n",
        "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out_int))\n",
        "        print(\"------------\")\n",
        "\n",
        "lists = range(0, iters_num, plot_interval)\n",
        "plt.plot(lists, all_losses, label=\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iters:0\n",
            "Loss:5515359026.555471\n",
            "Pred:[207  84  39 219 172  47   4   0]\n",
            "True:[1 0 0 1 0 1 0 1]\n",
            "72 + 77 = 38196\n",
            "------------\n",
            "iters:100\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 1 0 0]\n",
            "90 + 10 = 0\n",
            "------------\n",
            "iters:200\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 0 1 1 1 1 0]\n",
            "21 + 9 = 0\n",
            "------------\n",
            "iters:300\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 0 0 0 0]\n",
            "114 + 78 = 0\n",
            "------------\n",
            "iters:400\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 1 0 0 0 0]\n",
            "10 + 38 = 0\n",
            "------------\n",
            "iters:500\n",
            "Loss:3.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 1 1 1 1]\n",
            "19 + 76 = 0\n",
            "------------\n",
            "iters:600\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 1 0 1 1]\n",
            "81 + 122 = 0\n",
            "------------\n",
            "iters:700\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 1 0 0 0]\n",
            "46 + 74 = 0\n",
            "------------\n",
            "iters:800\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 1 1 1 1]\n",
            "105 + 38 = 0\n",
            "------------\n",
            "iters:900\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 1 1 0 0]\n",
            "86 + 70 = 0\n",
            "------------\n",
            "iters:1000\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 1 1 0 1]\n",
            "111 + 46 = 0\n",
            "------------\n",
            "iters:1100\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 1 1 1 0]\n",
            "115 + 59 = 0\n",
            "------------\n",
            "iters:1200\n",
            "Loss:0.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 0 0]\n",
            "79 + 49 = 0\n",
            "------------\n",
            "iters:1300\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 1 1 0 0 0]\n",
            "6 + 50 = 0\n",
            "------------\n",
            "iters:1400\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 1 0 1 1]\n",
            "113 + 42 = 0\n",
            "------------\n",
            "iters:1500\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 0 0 1 0]\n",
            "71 + 123 = 0\n",
            "------------\n",
            "iters:1600\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 0 1 1 1]\n",
            "51 + 20 = 0\n",
            "------------\n",
            "iters:1700\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 1 1 0 0 0]\n",
            "11 + 45 = 0\n",
            "------------\n",
            "iters:1800\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 1 0 0 0]\n",
            "41 + 31 = 0\n",
            "------------\n",
            "iters:1900\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 0 1 0 0]\n",
            "109 + 39 = 0\n",
            "------------\n",
            "iters:2000\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 1 0 0 1]\n",
            "17 + 88 = 0\n",
            "------------\n",
            "iters:2100\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 0 0 0 1]\n",
            "93 + 84 = 0\n",
            "------------\n",
            "iters:2200\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 0 0 1 1 1 0]\n",
            "7 + 7 = 0\n",
            "------------\n",
            "iters:2300\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 0 0 1 1]\n",
            "108 + 7 = 0\n",
            "------------\n",
            "iters:2400\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 1 0 0 0]\n",
            "42 + 94 = 0\n",
            "------------\n",
            "iters:2500\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 1 0 0]\n",
            "107 + 57 = 0\n",
            "------------\n",
            "iters:2600\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 0 1]\n",
            "56 + 73 = 0\n",
            "------------\n",
            "iters:2700\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 0 0 1 0]\n",
            "57 + 121 = 0\n",
            "------------\n",
            "iters:2800\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 1 0 0 0]\n",
            "65 + 23 = 0\n",
            "------------\n",
            "iters:2900\n",
            "Loss:3.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 1 1 1 1]\n",
            "28 + 83 = 0\n",
            "------------\n",
            "iters:3000\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 1 0 0 0]\n",
            "57 + 79 = 0\n",
            "------------\n",
            "iters:3100\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 1 1 0]\n",
            "99 + 67 = 0\n",
            "------------\n",
            "iters:3200\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 0 1 0 1 1 1]\n",
            "12 + 11 = 0\n",
            "------------\n",
            "iters:3300\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 1 1 1]\n",
            "126 + 41 = 0\n",
            "------------\n",
            "iters:3400\n",
            "Loss:3.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 1 1 1 0]\n",
            "101 + 25 = 0\n",
            "------------\n",
            "iters:3500\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 1 1 0 1]\n",
            "64 + 109 = 0\n",
            "------------\n",
            "iters:3600\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 1 0 1 1]\n",
            "95 + 108 = 0\n",
            "------------\n",
            "iters:3700\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 1 0 1 0]\n",
            "3 + 119 = 0\n",
            "------------\n",
            "iters:3800\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 1 1 0 0]\n",
            "53 + 119 = 0\n",
            "------------\n",
            "iters:3900\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 0 1 1 1]\n",
            "42 + 45 = 0\n",
            "------------\n",
            "iters:4000\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 1 0 1 1]\n",
            "69 + 6 = 0\n",
            "------------\n",
            "iters:4100\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 0 0 0 1]\n",
            "30 + 115 = 0\n",
            "------------\n",
            "iters:4200\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 1 1 1 0]\n",
            "63 + 79 = 0\n",
            "------------\n",
            "iters:4300\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 0 0 0 1]\n",
            "94 + 19 = 0\n",
            "------------\n",
            "iters:4400\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 1 1 0 0]\n",
            "62 + 94 = 0\n",
            "------------\n",
            "iters:4500\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 1 0 0 1]\n",
            "65 + 8 = 0\n",
            "------------\n",
            "iters:4600\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 1 1 0 0]\n",
            "49 + 59 = 0\n",
            "------------\n",
            "iters:4700\n",
            "Loss:3.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 1 1 1 0]\n",
            "27 + 99 = 0\n",
            "------------\n",
            "iters:4800\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 1 0]\n",
            "124 + 6 = 0\n",
            "------------\n",
            "iters:4900\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 0 1 1 1 1]\n",
            "6 + 41 = 0\n",
            "------------\n",
            "iters:5000\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 0 0 1 1]\n",
            "95 + 84 = 0\n",
            "------------\n",
            "iters:5100\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 0 1 0 1]\n",
            "55 + 14 = 0\n",
            "------------\n",
            "iters:5200\n",
            "Loss:3.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 0 1 1 1]\n",
            "84 + 35 = 0\n",
            "------------\n",
            "iters:5300\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 0 0 1 1 0 0]\n",
            "5 + 7 = 0\n",
            "------------\n",
            "iters:5400\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 1 1 0 0]\n",
            "7 + 117 = 0\n",
            "------------\n",
            "iters:5500\n",
            "Loss:3.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 1 1 1 0 1]\n",
            "121 + 100 = 0\n",
            "------------\n",
            "iters:5600\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 0 1 1 0]\n",
            "71 + 15 = 0\n",
            "------------\n",
            "iters:5700\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 1 0 1 1]\n",
            "98 + 41 = 0\n",
            "------------\n",
            "iters:5800\n",
            "Loss:0.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 0 0]\n",
            "65 + 63 = 0\n",
            "------------\n",
            "iters:5900\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 0 1 1 0]\n",
            "4 + 114 = 0\n",
            "------------\n",
            "iters:6000\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 1 0 1 0]\n",
            "94 + 108 = 0\n",
            "------------\n",
            "iters:6100\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 1 0 1 0]\n",
            "80 + 42 = 0\n",
            "------------\n",
            "iters:6200\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 1 0 0 0]\n",
            "88 + 32 = 0\n",
            "------------\n",
            "iters:6300\n",
            "Loss:3.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 1 1 1 1]\n",
            "40 + 87 = 0\n",
            "------------\n",
            "iters:6400\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 0 0 1 0]\n",
            "0 + 66 = 0\n",
            "------------\n",
            "iters:6500\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 1 0 1 0]\n",
            "88 + 66 = 0\n",
            "------------\n",
            "iters:6600\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 0 0 1 0]\n",
            "45 + 37 = 0\n",
            "------------\n",
            "iters:6700\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 1 1 0 0]\n",
            "108 + 0 = 0\n",
            "------------\n",
            "iters:6800\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 0 1]\n",
            "30 + 99 = 0\n",
            "------------\n",
            "iters:6900\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 1 1 0]\n",
            "60 + 106 = 0\n",
            "------------\n",
            "iters:7000\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 1 0 0 1 0]\n",
            "21 + 29 = 0\n",
            "------------\n",
            "iters:7100\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 1 1 1 0 0]\n",
            "15 + 45 = 0\n",
            "------------\n",
            "iters:7200\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 1 1 0 0]\n",
            "17 + 107 = 0\n",
            "------------\n",
            "iters:7300\n",
            "Loss:0.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 0 0]\n",
            "65 + 63 = 0\n",
            "------------\n",
            "iters:7400\n",
            "Loss:0.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 0 0 0 0]\n",
            "60 + 4 = 0\n",
            "------------\n",
            "iters:7500\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 1 1 1 0 1]\n",
            "44 + 17 = 0\n",
            "------------\n",
            "iters:7600\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 0 1 1 0]\n",
            "13 + 57 = 0\n",
            "------------\n",
            "iters:7700\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 0 0 1]\n",
            "75 + 86 = 0\n",
            "------------\n",
            "iters:7800\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 1 0 1 0]\n",
            "69 + 101 = 0\n",
            "------------\n",
            "iters:7900\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 0 1 0]\n",
            "57 + 105 = 0\n",
            "------------\n",
            "iters:8000\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 0 0 0 0]\n",
            "109 + 35 = 0\n",
            "------------\n",
            "iters:8100\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 1 0 0 1]\n",
            "37 + 116 = 0\n",
            "------------\n",
            "iters:8200\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 1 1 1]\n",
            "42 + 125 = 0\n",
            "------------\n",
            "iters:8300\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 1 0 0 1]\n",
            "15 + 74 = 0\n",
            "------------\n",
            "iters:8400\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 1 1 0 1]\n",
            "21 + 56 = 0\n",
            "------------\n",
            "iters:8500\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 1 1 1 1]\n",
            "80 + 63 = 0\n",
            "------------\n",
            "iters:8600\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 1 1 0]\n",
            "48 + 118 = 0\n",
            "------------\n",
            "iters:8700\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 1 0 1]\n",
            "52 + 113 = 0\n",
            "------------\n",
            "iters:8800\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 1 0 0 0]\n",
            "64 + 88 = 0\n",
            "------------\n",
            "iters:8900\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 0 0 0 0]\n",
            "30 + 82 = 0\n",
            "------------\n",
            "iters:9000\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 0 0 1 1]\n",
            "34 + 49 = 0\n",
            "------------\n",
            "iters:9100\n",
            "Loss:3.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 1 0 1 1 0 1]\n",
            "110 + 127 = 0\n",
            "------------\n",
            "iters:9200\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 0 1 1 1]\n",
            "2 + 85 = 0\n",
            "------------\n",
            "iters:9300\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 1 0 1]\n",
            "118 + 47 = 0\n",
            "------------\n",
            "iters:9400\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 1 0 1]\n",
            "108 + 57 = 0\n",
            "------------\n",
            "iters:9500\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 0 1]\n",
            "117 + 12 = 0\n",
            "------------\n",
            "iters:9600\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 0 1 0 0 0]\n",
            "28 + 12 = 0\n",
            "------------\n",
            "iters:9700\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 1 0 0 1]\n",
            "88 + 49 = 0\n",
            "------------\n",
            "iters:9800\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 1 0 0 0]\n",
            "58 + 78 = 0\n",
            "------------\n",
            "iters:9900\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 1 0 0 1]\n",
            "34 + 55 = 0\n",
            "------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-rodnTxaD2k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5Q3vY0_aFW8"
      },
      "source": [
        "#tanh(numpyにtanhが用意されている。導関数をd_tanhとして作成しよう)\n",
        "\n",
        "中間層の活性化関数をtanhに変更し（かつHeの初期値を使用すると）かなり学習がスムーズに行われる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XibS5UPSaB5O",
        "outputId": "f25311af-d474-4354-f427-0183a3486d3d"
      },
      "source": [
        "import numpy as np\n",
        "from common import functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def d_tanh(x):\n",
        "  sinh = (np.exp(x) - np.exp(-x)) / 2\n",
        "  cosh = (np.exp(x) + np.exp(-x)) / 2\n",
        "  return (cosh**2 - sinh**2) / cosh**2\n",
        "\n",
        "# データを用意\n",
        "# 2進数の桁数\n",
        "binary_dim = 8\n",
        "# 最大値 + 1\n",
        "largest_number = pow(2, binary_dim)\n",
        "# largest_numberまで2進数を用意\n",
        "binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
        "\n",
        "input_layer_size = 2\n",
        "hidden_layer_size = 16\n",
        "output_layer_size = 1\n",
        "\n",
        "weight_init_std = 1\n",
        "learning_rate = 0.1\n",
        "\n",
        "iters_num = 10000\n",
        "plot_interval = 100\n",
        "\n",
        "# ウェイト初期化 (バイアスは簡単のため省略)\n",
        "W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n",
        "W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n",
        "W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n",
        "\n",
        "# Xavier\n",
        "\n",
        "\n",
        "# He\n",
        "W_in = np.random.randn(input_layer_size, hidden_layer_size) / np.sqrt(input_layer_size) * np.sqrt(2)\n",
        "W_out = np.random.randn(hidden_layer_size, output_layer_size) / np.sqrt(hidden_layer_size) * np.sqrt(2)\n",
        "W = np.random.randn(hidden_layer_size, hidden_layer_size) / np.sqrt(hidden_layer_size) * np.sqrt(2)\n",
        "\n",
        "\n",
        "# 勾配\n",
        "W_in_grad = np.zeros_like(W_in)\n",
        "W_out_grad = np.zeros_like(W_out)\n",
        "W_grad = np.zeros_like(W)\n",
        "\n",
        "u = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "z = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "y = np.zeros((output_layer_size, binary_dim))\n",
        "\n",
        "delta_out = np.zeros((output_layer_size, binary_dim))\n",
        "delta = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "\n",
        "all_losses = []\n",
        "\n",
        "for i in range(iters_num):\n",
        "    \n",
        "    # A, B初期化 (a + b = d)\n",
        "    a_int = np.random.randint(largest_number/2)\n",
        "    a_bin = binary[a_int] # binary encoding\n",
        "    b_int = np.random.randint(largest_number/2)\n",
        "    b_bin = binary[b_int] # binary encoding\n",
        "    \n",
        "    # 正解データ\n",
        "    d_int = a_int + b_int\n",
        "    d_bin = binary[d_int]\n",
        "    \n",
        "    # 出力バイナリ\n",
        "    out_bin = np.zeros_like(d_bin)\n",
        "    \n",
        "    # 時系列全体の誤差\n",
        "    all_loss = 0    \n",
        "    \n",
        "    # 時系列ループ\n",
        "    for t in range(binary_dim):\n",
        "        # 入力値\n",
        "        X = np.array([a_bin[ - t - 1], b_bin[ - t - 1]]).reshape(1, -1)\n",
        "        # 時刻tにおける正解データ\n",
        "        dd = np.array([d_bin[binary_dim - t - 1]])\n",
        "        \n",
        "        u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n",
        "        # z[:,t+1] = functions.sigmoid(u[:,t+1])\n",
        "        z[:,t+1] = np.tanh(u[:,t+1])\n",
        "\n",
        "        # y[:,t] = functions.sigmoid(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
        "        y[:,t] = np.tanh(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
        "\n",
        "        #誤差\n",
        "        loss = functions.mean_squared_error(dd, y[:,t])\n",
        "        \n",
        "        # delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * functions.d_sigmoid(y[:,t])        \n",
        "        delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * d_tanh(y[:,t])    \n",
        "\n",
        "        all_loss += loss\n",
        "\n",
        "        out_bin[binary_dim - t - 1] = np.round(y[:,t])\n",
        "    \n",
        "    \n",
        "    for t in range(binary_dim)[::-1]:\n",
        "        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n",
        "\n",
        "        # delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_sigmoid(u[:,t+1])\n",
        "        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * d_tanh(u[:,t+1])\n",
        "\n",
        "        # 勾配更新\n",
        "        W_out_grad += np.dot(z[:,t+1].reshape(-1,1), delta_out[:,t].reshape(-1,1))\n",
        "        W_grad += np.dot(z[:,t].reshape(-1,1), delta[:,t].reshape(1,-1))\n",
        "        W_in_grad += np.dot(X.T, delta[:,t].reshape(1,-1))\n",
        "    \n",
        "    # 勾配適用\n",
        "    W_in -= learning_rate * W_in_grad\n",
        "    W_out -= learning_rate * W_out_grad\n",
        "    W -= learning_rate * W_grad\n",
        "    \n",
        "    W_in_grad *= 0\n",
        "    W_out_grad *= 0\n",
        "    W_grad *= 0\n",
        "    \n",
        "\n",
        "    if(i % plot_interval == 0):\n",
        "        all_losses.append(all_loss)        \n",
        "        print(\"iters:\" + str(i))\n",
        "        print(\"Loss:\" + str(all_loss))\n",
        "        print(\"Pred:\" + str(out_bin))\n",
        "        print(\"True:\" + str(d_bin))\n",
        "        out_int = 0\n",
        "        for index,x in enumerate(reversed(out_bin)):\n",
        "            out_int += x * pow(2, index)\n",
        "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out_int))\n",
        "        print(\"------------\")\n",
        "\n",
        "lists = range(0, iters_num, plot_interval)\n",
        "plt.plot(lists, all_losses, label=\"loss\")\n",
        "plt.show()\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iters:0\n",
            "Loss:0.2190988033016282\n",
            "Pred:[0 0 0 0 0 1 0 0]\n",
            "True:[1 0 0 0 0 1 0 0]\n",
            "10 + 122 = 4\n",
            "------------\n",
            "iters:100\n",
            "Loss:0.7908142641178675\n",
            "Pred:[1 1 1 1 1 0 1 1]\n",
            "True:[0 1 1 1 1 0 1 1]\n",
            "71 + 52 = 251\n",
            "------------\n",
            "iters:200\n",
            "Loss:0.9899452491316725\n",
            "Pred:[255   0   1   1   1   1   1   0]\n",
            "True:[0 0 1 0 0 1 1 0]\n",
            "26 + 12 = 32702\n",
            "------------\n",
            "iters:300\n",
            "Loss:1.9319003959712298\n",
            "Pred:[0 1 1 1 1 1 1 1]\n",
            "True:[1 0 0 1 1 0 0 0]\n",
            "57 + 95 = 127\n",
            "------------\n",
            "iters:400\n",
            "Loss:1.190620154945363\n",
            "Pred:[0 0 0 0 1 0 0 1]\n",
            "True:[0 1 1 1 1 0 0 1]\n",
            "59 + 62 = 9\n",
            "------------\n",
            "iters:500\n",
            "Loss:0.32441574949623975\n",
            "Pred:[0 1 1 0 1 1 0 1]\n",
            "True:[0 1 1 0 0 1 0 1]\n",
            "53 + 48 = 109\n",
            "------------\n",
            "iters:600\n",
            "Loss:0.15147648035387035\n",
            "Pred:[1 1 1 0 1 0 1 1]\n",
            "True:[1 1 1 0 1 0 1 1]\n",
            "109 + 126 = 235\n",
            "------------\n",
            "iters:700\n",
            "Loss:0.14354302365589097\n",
            "Pred:[1 0 0 1 0 1 1 1]\n",
            "True:[1 0 0 1 0 1 1 1]\n",
            "83 + 68 = 151\n",
            "------------\n",
            "iters:800\n",
            "Loss:0.06185582753848629\n",
            "Pred:[0 0 0 1 1 1 0 0]\n",
            "True:[0 0 0 1 1 1 0 0]\n",
            "21 + 7 = 28\n",
            "------------\n",
            "iters:900\n",
            "Loss:0.0967476708214489\n",
            "Pred:[0 1 0 1 0 1 1 0]\n",
            "True:[0 1 0 1 0 1 1 0]\n",
            "28 + 58 = 86\n",
            "------------\n",
            "iters:1000\n",
            "Loss:0.018003225910141543\n",
            "Pred:[0 1 1 1 1 0 0 0]\n",
            "True:[0 1 1 1 1 0 0 0]\n",
            "23 + 97 = 120\n",
            "------------\n",
            "iters:1100\n",
            "Loss:0.008409313806182721\n",
            "Pred:[0 1 0 1 1 1 0 1]\n",
            "True:[0 1 0 1 1 1 0 1]\n",
            "68 + 25 = 93\n",
            "------------\n",
            "iters:1200\n",
            "Loss:0.05156752845687809\n",
            "Pred:[1 0 0 1 1 1 1 0]\n",
            "True:[1 0 0 1 1 1 1 0]\n",
            "115 + 43 = 158\n",
            "------------\n",
            "iters:1300\n",
            "Loss:0.003197710325773356\n",
            "Pred:[0 0 1 0 1 1 0 0]\n",
            "True:[0 0 1 0 1 1 0 0]\n",
            "12 + 32 = 44\n",
            "------------\n",
            "iters:1400\n",
            "Loss:0.005968512237692891\n",
            "Pred:[1 0 1 0 1 1 0 1]\n",
            "True:[1 0 1 0 1 1 0 1]\n",
            "125 + 48 = 173\n",
            "------------\n",
            "iters:1500\n",
            "Loss:0.04924804024884991\n",
            "Pred:[0 1 1 1 1 0 1 1]\n",
            "True:[0 1 1 1 1 0 1 1]\n",
            "102 + 21 = 123\n",
            "------------\n",
            "iters:1600\n",
            "Loss:0.032146637339714366\n",
            "Pred:[0 0 1 1 0 1 1 1]\n",
            "True:[0 0 1 1 0 1 1 1]\n",
            "26 + 29 = 55\n",
            "------------\n",
            "iters:1700\n",
            "Loss:0.0006251600850064027\n",
            "Pred:[1 0 0 1 1 1 1 1]\n",
            "True:[1 0 0 1 1 1 1 1]\n",
            "77 + 82 = 159\n",
            "------------\n",
            "iters:1800\n",
            "Loss:0.0021003978893000264\n",
            "Pred:[1 1 0 1 0 0 0 0]\n",
            "True:[1 1 0 1 0 0 0 0]\n",
            "91 + 117 = 208\n",
            "------------\n",
            "iters:1900\n",
            "Loss:0.026019224985582286\n",
            "Pred:[0 1 0 0 1 0 1 1]\n",
            "True:[0 1 0 0 1 0 1 1]\n",
            "28 + 47 = 75\n",
            "------------\n",
            "iters:2000\n",
            "Loss:0.005459480169237155\n",
            "Pred:[0 1 1 1 1 0 0 0]\n",
            "True:[0 1 1 1 1 0 0 0]\n",
            "110 + 10 = 120\n",
            "------------\n",
            "iters:2100\n",
            "Loss:0.010571195818599723\n",
            "Pred:[0 1 0 1 0 0 1 0]\n",
            "True:[0 1 0 1 0 0 1 0]\n",
            "40 + 42 = 82\n",
            "------------\n",
            "iters:2200\n",
            "Loss:0.024191481841656367\n",
            "Pred:[1 1 0 0 1 0 0 0]\n",
            "True:[1 1 0 0 1 0 0 0]\n",
            "119 + 81 = 200\n",
            "------------\n",
            "iters:2300\n",
            "Loss:0.07577726528245768\n",
            "Pred:[1 0 0 0 1 1 1 1]\n",
            "True:[1 0 0 0 1 1 1 1]\n",
            "17 + 126 = 143\n",
            "------------\n",
            "iters:2400\n",
            "Loss:0.0020531104348327366\n",
            "Pred:[1 0 0 1 0 1 1 0]\n",
            "True:[1 0 0 1 0 1 1 0]\n",
            "27 + 123 = 150\n",
            "------------\n",
            "iters:2500\n",
            "Loss:0.0010694551328693286\n",
            "Pred:[0 0 1 1 0 0 0 0]\n",
            "True:[0 0 1 1 0 0 0 0]\n",
            "17 + 31 = 48\n",
            "------------\n",
            "iters:2600\n",
            "Loss:0.005309966784201097\n",
            "Pred:[1 0 0 0 1 0 0 0]\n",
            "True:[1 0 0 0 1 0 0 0]\n",
            "13 + 123 = 136\n",
            "------------\n",
            "iters:2700\n",
            "Loss:0.0011513141289588586\n",
            "Pred:[0 1 0 1 1 1 0 0]\n",
            "True:[0 1 0 1 1 1 0 0]\n",
            "31 + 61 = 92\n",
            "------------\n",
            "iters:2800\n",
            "Loss:0.00046556213127716957\n",
            "Pred:[0 0 1 1 1 1 0 0]\n",
            "True:[0 0 1 1 1 1 0 0]\n",
            "12 + 48 = 60\n",
            "------------\n",
            "iters:2900\n",
            "Loss:0.006070133606310664\n",
            "Pred:[1 0 0 0 0 0 1 0]\n",
            "True:[1 0 0 0 0 0 1 0]\n",
            "24 + 106 = 130\n",
            "------------\n",
            "iters:3000\n",
            "Loss:0.006526633404426272\n",
            "Pred:[0 0 0 1 1 0 1 1]\n",
            "True:[0 0 0 1 1 0 1 1]\n",
            "1 + 26 = 27\n",
            "------------\n",
            "iters:3100\n",
            "Loss:0.004002576547857237\n",
            "Pred:[0 1 0 1 0 0 1 1]\n",
            "True:[0 1 0 1 0 0 1 1]\n",
            "60 + 23 = 83\n",
            "------------\n",
            "iters:3200\n",
            "Loss:0.002618849931679613\n",
            "Pred:[1 1 0 0 0 1 1 1]\n",
            "True:[1 1 0 0 0 1 1 1]\n",
            "119 + 80 = 199\n",
            "------------\n",
            "iters:3300\n",
            "Loss:0.0033356238939502006\n",
            "Pred:[0 1 0 1 0 1 0 1]\n",
            "True:[0 1 0 1 0 1 0 1]\n",
            "81 + 4 = 85\n",
            "------------\n",
            "iters:3400\n",
            "Loss:0.0012629860383188809\n",
            "Pred:[0 1 0 1 0 1 1 0]\n",
            "True:[0 1 0 1 0 1 1 0]\n",
            "1 + 85 = 86\n",
            "------------\n",
            "iters:3500\n",
            "Loss:0.002659655953940865\n",
            "Pred:[0 1 1 1 0 0 0 0]\n",
            "True:[0 1 1 1 0 0 0 0]\n",
            "109 + 3 = 112\n",
            "------------\n",
            "iters:3600\n",
            "Loss:0.0033808424587213875\n",
            "Pred:[0 0 0 1 1 0 0 1]\n",
            "True:[0 0 0 1 1 0 0 1]\n",
            "14 + 11 = 25\n",
            "------------\n",
            "iters:3700\n",
            "Loss:0.00452282585261379\n",
            "Pred:[1 0 0 1 1 1 0 0]\n",
            "True:[1 0 0 1 1 1 0 0]\n",
            "97 + 59 = 156\n",
            "------------\n",
            "iters:3800\n",
            "Loss:0.0018350170275128986\n",
            "Pred:[1 1 1 0 0 1 0 1]\n",
            "True:[1 1 1 0 0 1 0 1]\n",
            "118 + 111 = 229\n",
            "------------\n",
            "iters:3900\n",
            "Loss:1.1041744082644988e-05\n",
            "Pred:[0 1 1 0 1 1 0 0]\n",
            "True:[0 1 1 0 1 1 0 0]\n",
            "85 + 23 = 108\n",
            "------------\n",
            "iters:4000\n",
            "Loss:0.0008498490300707199\n",
            "Pred:[1 1 0 1 1 1 1 1]\n",
            "True:[1 1 0 1 1 1 1 1]\n",
            "99 + 124 = 223\n",
            "------------\n",
            "iters:4100\n",
            "Loss:0.006920909248749626\n",
            "Pred:[1 0 0 1 1 0 0 0]\n",
            "True:[1 0 0 1 1 0 0 0]\n",
            "52 + 100 = 152\n",
            "------------\n",
            "iters:4200\n",
            "Loss:0.00367433811469142\n",
            "Pred:[1 0 0 1 0 1 0 0]\n",
            "True:[1 0 0 1 0 1 0 0]\n",
            "86 + 62 = 148\n",
            "------------\n",
            "iters:4300\n",
            "Loss:0.040273102363032276\n",
            "Pred:[0 0 1 0 0 0 0 0]\n",
            "True:[0 0 1 0 0 0 0 0]\n",
            "25 + 7 = 32\n",
            "------------\n",
            "iters:4400\n",
            "Loss:0.010597503375656861\n",
            "Pred:[0 1 0 0 0 1 1 0]\n",
            "True:[0 1 0 0 0 1 1 0]\n",
            "26 + 44 = 70\n",
            "------------\n",
            "iters:4500\n",
            "Loss:0.000869187534304967\n",
            "Pred:[0 1 0 0 1 0 0 0]\n",
            "True:[0 1 0 0 1 0 0 0]\n",
            "71 + 1 = 72\n",
            "------------\n",
            "iters:4600\n",
            "Loss:7.60320098926538e-05\n",
            "Pred:[0 1 1 0 1 1 1 1]\n",
            "True:[0 1 1 0 1 1 1 1]\n",
            "3 + 108 = 111\n",
            "------------\n",
            "iters:4700\n",
            "Loss:0.009238411470039173\n",
            "Pred:[1 0 0 1 0 1 0 1]\n",
            "True:[1 0 0 1 0 1 0 1]\n",
            "89 + 60 = 149\n",
            "------------\n",
            "iters:4800\n",
            "Loss:0.0004455266408764563\n",
            "Pred:[0 1 0 0 1 1 0 0]\n",
            "True:[0 1 0 0 1 1 0 0]\n",
            "24 + 52 = 76\n",
            "------------\n",
            "iters:4900\n",
            "Loss:0.0003805009049617117\n",
            "Pred:[0 1 1 1 0 1 0 1]\n",
            "True:[0 1 1 1 0 1 0 1]\n",
            "117 + 0 = 117\n",
            "------------\n",
            "iters:5000\n",
            "Loss:0.002151556133215176\n",
            "Pred:[0 1 0 1 0 1 0 0]\n",
            "True:[0 1 0 1 0 1 0 0]\n",
            "10 + 74 = 84\n",
            "------------\n",
            "iters:5100\n",
            "Loss:0.00010023016345878563\n",
            "Pred:[0 1 1 1 1 1 1 0]\n",
            "True:[0 1 1 1 1 1 1 0]\n",
            "38 + 88 = 126\n",
            "------------\n",
            "iters:5200\n",
            "Loss:0.00031674623212044924\n",
            "Pred:[0 0 0 0 1 1 1 1]\n",
            "True:[0 0 0 0 1 1 1 1]\n",
            "2 + 13 = 15\n",
            "------------\n",
            "iters:5300\n",
            "Loss:0.0048269496939126535\n",
            "Pred:[0 1 1 1 0 0 1 1]\n",
            "True:[0 1 1 1 0 0 1 1]\n",
            "86 + 29 = 115\n",
            "------------\n",
            "iters:5400\n",
            "Loss:0.0011276988647830448\n",
            "Pred:[1 0 0 1 1 1 0 1]\n",
            "True:[1 0 0 1 1 1 0 1]\n",
            "68 + 89 = 157\n",
            "------------\n",
            "iters:5500\n",
            "Loss:0.004951370172533734\n",
            "Pred:[0 1 0 1 1 0 1 0]\n",
            "True:[0 1 0 1 1 0 1 0]\n",
            "4 + 86 = 90\n",
            "------------\n",
            "iters:5600\n",
            "Loss:0.0026294146123594826\n",
            "Pred:[0 1 1 0 0 0 1 1]\n",
            "True:[0 1 1 0 0 0 1 1]\n",
            "31 + 68 = 99\n",
            "------------\n",
            "iters:5700\n",
            "Loss:7.108051237693091e-05\n",
            "Pred:[1 1 0 1 1 1 1 0]\n",
            "True:[1 1 0 1 1 1 1 0]\n",
            "113 + 109 = 222\n",
            "------------\n",
            "iters:5800\n",
            "Loss:0.004530479318090302\n",
            "Pred:[0 1 1 0 1 1 1 0]\n",
            "True:[0 1 1 0 1 1 1 0]\n",
            "92 + 18 = 110\n",
            "------------\n",
            "iters:5900\n",
            "Loss:0.0013415766542973376\n",
            "Pred:[1 0 1 0 0 1 0 1]\n",
            "True:[1 0 1 0 0 1 0 1]\n",
            "76 + 89 = 165\n",
            "------------\n",
            "iters:6000\n",
            "Loss:0.15534953083005484\n",
            "Pred:[1 0 0 0 0 1 0 1]\n",
            "True:[1 0 0 0 0 1 0 1]\n",
            "7 + 126 = 133\n",
            "------------\n",
            "iters:6100\n",
            "Loss:0.0014717606205249484\n",
            "Pred:[1 0 0 1 0 1 0 0]\n",
            "True:[1 0 0 1 0 1 0 0]\n",
            "90 + 58 = 148\n",
            "------------\n",
            "iters:6200\n",
            "Loss:0.0005805940796675948\n",
            "Pred:[0 1 0 1 1 0 1 0]\n",
            "True:[0 1 0 1 1 0 1 0]\n",
            "5 + 85 = 90\n",
            "------------\n",
            "iters:6300\n",
            "Loss:0.0006322103143097301\n",
            "Pred:[1 1 0 1 1 0 0 0]\n",
            "True:[1 1 0 1 1 0 0 0]\n",
            "92 + 124 = 216\n",
            "------------\n",
            "iters:6400\n",
            "Loss:0.0005136356524005806\n",
            "Pred:[0 0 0 1 0 0 0 1]\n",
            "True:[0 0 0 1 0 0 0 1]\n",
            "12 + 5 = 17\n",
            "------------\n",
            "iters:6500\n",
            "Loss:0.0005214881683972781\n",
            "Pred:[0 0 0 1 1 1 1 0]\n",
            "True:[0 0 0 1 1 1 1 0]\n",
            "26 + 4 = 30\n",
            "------------\n",
            "iters:6600\n",
            "Loss:0.0018424634326173613\n",
            "Pred:[1 0 1 1 0 1 0 0]\n",
            "True:[1 0 1 1 0 1 0 0]\n",
            "118 + 62 = 180\n",
            "------------\n",
            "iters:6700\n",
            "Loss:0.0011855725587994934\n",
            "Pred:[1 0 0 1 0 1 1 0]\n",
            "True:[1 0 0 1 0 1 1 0]\n",
            "78 + 72 = 150\n",
            "------------\n",
            "iters:6800\n",
            "Loss:0.006301854603971334\n",
            "Pred:[1 1 0 0 1 1 1 0]\n",
            "True:[1 1 0 0 1 1 1 0]\n",
            "88 + 118 = 206\n",
            "------------\n",
            "iters:6900\n",
            "Loss:0.0003887981602879311\n",
            "Pred:[0 1 0 1 1 0 1 1]\n",
            "True:[0 1 0 1 1 0 1 1]\n",
            "15 + 76 = 91\n",
            "------------\n",
            "iters:7000\n",
            "Loss:0.001579568993405223\n",
            "Pred:[1 0 0 0 1 1 1 0]\n",
            "True:[1 0 0 0 1 1 1 0]\n",
            "58 + 84 = 142\n",
            "------------\n",
            "iters:7100\n",
            "Loss:0.00016940693465536277\n",
            "Pred:[0 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 1 1 1 1 1]\n",
            "75 + 52 = 127\n",
            "------------\n",
            "iters:7200\n",
            "Loss:0.0004837501770845414\n",
            "Pred:[1 0 0 1 0 1 0 0]\n",
            "True:[1 0 0 1 0 1 0 0]\n",
            "28 + 120 = 148\n",
            "------------\n",
            "iters:7300\n",
            "Loss:0.001161073629390116\n",
            "Pred:[0 1 0 1 1 0 0 0]\n",
            "True:[0 1 0 1 1 0 0 0]\n",
            "7 + 81 = 88\n",
            "------------\n",
            "iters:7400\n",
            "Loss:0.0007373882177959312\n",
            "Pred:[1 0 0 0 1 1 0 1]\n",
            "True:[1 0 0 0 1 1 0 1]\n",
            "67 + 74 = 141\n",
            "------------\n",
            "iters:7500\n",
            "Loss:0.005134407979567475\n",
            "Pred:[1 0 1 0 0 1 0 1]\n",
            "True:[1 0 1 0 0 1 0 1]\n",
            "86 + 79 = 165\n",
            "------------\n",
            "iters:7600\n",
            "Loss:0.002316548537059099\n",
            "Pred:[1 0 0 1 0 1 0 0]\n",
            "True:[1 0 0 1 0 1 0 0]\n",
            "114 + 34 = 148\n",
            "------------\n",
            "iters:7700\n",
            "Loss:0.00017101822491300656\n",
            "Pred:[1 0 1 1 0 0 0 0]\n",
            "True:[1 0 1 1 0 0 0 0]\n",
            "113 + 63 = 176\n",
            "------------\n",
            "iters:7800\n",
            "Loss:0.0013390451703332558\n",
            "Pred:[0 0 1 0 1 0 1 0]\n",
            "True:[0 0 1 0 1 0 1 0]\n",
            "2 + 40 = 42\n",
            "------------\n",
            "iters:7900\n",
            "Loss:0.0006322523456176751\n",
            "Pred:[0 0 0 0 1 1 0 0]\n",
            "True:[0 0 0 0 1 1 0 0]\n",
            "11 + 1 = 12\n",
            "------------\n",
            "iters:8000\n",
            "Loss:0.0007396677731131472\n",
            "Pred:[0 1 1 0 1 1 0 1]\n",
            "True:[0 1 1 0 1 1 0 1]\n",
            "47 + 62 = 109\n",
            "------------\n",
            "iters:8100\n",
            "Loss:0.0030605648869880494\n",
            "Pred:[1 0 1 1 0 1 0 0]\n",
            "True:[1 0 1 1 0 1 0 0]\n",
            "67 + 113 = 180\n",
            "------------\n",
            "iters:8200\n",
            "Loss:0.0003989647520929447\n",
            "Pred:[1 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 0 0]\n",
            "5 + 123 = 128\n",
            "------------\n",
            "iters:8300\n",
            "Loss:0.014714275751515288\n",
            "Pred:[0 0 0 1 0 1 1 0]\n",
            "True:[0 0 0 1 0 1 1 0]\n",
            "2 + 20 = 22\n",
            "------------\n",
            "iters:8400\n",
            "Loss:2.1426401734409842e-05\n",
            "Pred:[1 1 0 1 0 0 1 1]\n",
            "True:[1 1 0 1 0 0 1 1]\n",
            "127 + 84 = 211\n",
            "------------\n",
            "iters:8500\n",
            "Loss:0.001962531390284237\n",
            "Pred:[1 1 0 1 0 0 0 0]\n",
            "True:[1 1 0 1 0 0 0 0]\n",
            "100 + 108 = 208\n",
            "------------\n",
            "iters:8600\n",
            "Loss:0.0032760425649717216\n",
            "Pred:[1 0 1 0 1 0 1 1]\n",
            "True:[1 0 1 0 1 0 1 1]\n",
            "106 + 65 = 171\n",
            "------------\n",
            "iters:8700\n",
            "Loss:0.011669569303637224\n",
            "Pred:[0 0 1 0 1 0 0 1]\n",
            "True:[0 0 1 0 1 0 0 1]\n",
            "16 + 25 = 41\n",
            "------------\n",
            "iters:8800\n",
            "Loss:0.00023934911516248843\n",
            "Pred:[0 1 1 1 1 0 0 1]\n",
            "True:[0 1 1 1 1 0 0 1]\n",
            "107 + 14 = 121\n",
            "------------\n",
            "iters:8900\n",
            "Loss:0.001035754973121388\n",
            "Pred:[0 1 0 1 0 1 0 0]\n",
            "True:[0 1 0 1 0 1 0 0]\n",
            "41 + 43 = 84\n",
            "------------\n",
            "iters:9000\n",
            "Loss:0.0004165265292800819\n",
            "Pred:[1 0 1 0 0 0 0 1]\n",
            "True:[1 0 1 0 0 0 0 1]\n",
            "78 + 83 = 161\n",
            "------------\n",
            "iters:9100\n",
            "Loss:0.0004392680194751927\n",
            "Pred:[1 0 1 0 1 1 0 0]\n",
            "True:[1 0 1 0 1 1 0 0]\n",
            "123 + 49 = 172\n",
            "------------\n",
            "iters:9200\n",
            "Loss:0.009278328864979254\n",
            "Pred:[0 1 0 0 0 0 0 1]\n",
            "True:[0 1 0 0 0 0 0 1]\n",
            "22 + 43 = 65\n",
            "------------\n",
            "iters:9300\n",
            "Loss:0.0008982660268173829\n",
            "Pred:[0 1 0 0 1 1 1 1]\n",
            "True:[0 1 0 0 1 1 1 1]\n",
            "21 + 58 = 79\n",
            "------------\n",
            "iters:9400\n",
            "Loss:0.0022826552448860387\n",
            "Pred:[1 0 0 1 0 1 0 0]\n",
            "True:[1 0 0 1 0 1 0 0]\n",
            "116 + 32 = 148\n",
            "------------\n",
            "iters:9500\n",
            "Loss:0.003538455158767142\n",
            "Pred:[0 0 1 1 0 0 1 1]\n",
            "True:[0 0 1 1 0 0 1 1]\n",
            "30 + 21 = 51\n",
            "------------\n",
            "iters:9600\n",
            "Loss:0.004900269500056961\n",
            "Pred:[1 0 1 0 1 1 1 0]\n",
            "True:[1 0 1 0 1 1 1 0]\n",
            "76 + 98 = 174\n",
            "------------\n",
            "iters:9700\n",
            "Loss:0.0009280486905462543\n",
            "Pred:[1 0 0 1 1 0 0 1]\n",
            "True:[1 0 0 1 1 0 0 1]\n",
            "93 + 60 = 153\n",
            "------------\n",
            "iters:9800\n",
            "Loss:0.00012698111276786817\n",
            "Pred:[0 0 1 1 1 0 0 1]\n",
            "True:[0 0 1 1 1 0 0 1]\n",
            "20 + 37 = 57\n",
            "------------\n",
            "iters:9900\n",
            "Loss:0.0019135966743354489\n",
            "Pred:[0 0 1 1 1 0 1 0]\n",
            "True:[0 0 1 1 1 0 1 0]\n",
            "50 + 8 = 58\n",
            "------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwbXpICXaJeM"
      },
      "source": [
        "2000回で十分。"
      ]
    }
  ]
}