{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "3_1_simple_RNN_after.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MasahitoKatsuki/RabbitChallenge/blob/main/3_1_simple_RNN_after.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cNl2QA_Rnv5"
      },
      "source": [
        "# 準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkwjN1jNVAYy"
      },
      "source": [
        "## Googleドライブのマウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvFXpiH3EVC1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af566ff5-7214-4b55-bf7d-4104d361e2e9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ub7RYdeY6pK"
      },
      "source": [
        "## sys.pathの設定"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oql7L19rEsWi"
      },
      "source": [
        "以下では，Googleドライブのマイドライブ直下にDNN_codeフォルダを置くことを仮定しています．必要に応じて，パスを変更してください．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ic2JzkvFX59"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/DNN_code')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzGmsHRwO-bi"
      },
      "source": [
        "# simple RNN after\n",
        "### バイナリ加算"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "KNSG0aKXO-bk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0161552c-7ed9-41e4-8bec-c63f754da737"
      },
      "source": [
        "import numpy as np\n",
        "from common import functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def d_tanh(x):\n",
        "    return 1/(np.cosh(x) ** 2)\n",
        "\n",
        "# データを用意\n",
        "# 2進数の桁数\n",
        "binary_dim = 8\n",
        "# 最大値 + 1\n",
        "largest_number = pow(2, binary_dim)\n",
        "# largest_numberまで2進数を用意\n",
        "binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
        "\n",
        "input_layer_size = 2\n",
        "hidden_layer_size = 16\n",
        "output_layer_size = 1\n",
        "\n",
        "weight_init_std = 1\n",
        "learning_rate = 0.1\n",
        "\n",
        "iters_num = 10000\n",
        "plot_interval = 100\n",
        "\n",
        "# ウェイト初期化 (バイアスは簡単のため省略)\n",
        "W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n",
        "W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n",
        "W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n",
        "# Xavier\n",
        "# W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size))\n",
        "# W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "# W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "# He\n",
        "# W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size)) * np.sqrt(2)\n",
        "# W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n",
        "# W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n",
        "\n",
        "\n",
        "# 勾配\n",
        "W_in_grad = np.zeros_like(W_in)\n",
        "W_out_grad = np.zeros_like(W_out)\n",
        "W_grad = np.zeros_like(W)\n",
        "\n",
        "u = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "z = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "y = np.zeros((output_layer_size, binary_dim))\n",
        "\n",
        "delta_out = np.zeros((output_layer_size, binary_dim))\n",
        "delta = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "\n",
        "all_losses = []\n",
        "\n",
        "for i in range(iters_num):\n",
        "    \n",
        "    # A, B初期化 (a + b = d)\n",
        "    a_int = np.random.randint(largest_number/2)\n",
        "    a_bin = binary[a_int] # binary encoding\n",
        "    b_int = np.random.randint(largest_number/2)\n",
        "    b_bin = binary[b_int] # binary encoding\n",
        "    \n",
        "    # 正解データ\n",
        "    d_int = a_int + b_int\n",
        "    d_bin = binary[d_int]\n",
        "    \n",
        "    # 出力バイナリ\n",
        "    out_bin = np.zeros_like(d_bin)\n",
        "    \n",
        "    # 時系列全体の誤差\n",
        "    all_loss = 0    \n",
        "    \n",
        "    # 時系列ループ\n",
        "    for t in range(binary_dim):\n",
        "        # 入力値\n",
        "        X = np.array([a_bin[ - t - 1], b_bin[ - t - 1]]).reshape(1, -1)\n",
        "        # 時刻tにおける正解データ\n",
        "        dd = np.array([d_bin[binary_dim - t - 1]])\n",
        "        \n",
        "        u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n",
        "        z[:,t+1] = functions.sigmoid(u[:,t+1])\n",
        "#         z[:,t+1] = functions.relu(u[:,t+1])\n",
        "#         z[:,t+1] = np.tanh(u[:,t+1])    \n",
        "        y[:,t] = functions.sigmoid(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
        "\n",
        "\n",
        "        #誤差\n",
        "        loss = functions.mean_squared_error(dd, y[:,t])\n",
        "        \n",
        "        delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * functions.d_sigmoid(y[:,t])        \n",
        "        \n",
        "        all_loss += loss\n",
        "\n",
        "        out_bin[binary_dim - t - 1] = np.round(y[:,t])\n",
        "    \n",
        "    \n",
        "    for t in range(binary_dim)[::-1]:\n",
        "        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n",
        "\n",
        "        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_sigmoid(u[:,t+1])\n",
        "#         delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_relu(u[:,t+1])\n",
        "#         delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * d_tanh(u[:,t+1])    \n",
        "\n",
        "        # 勾配更新\n",
        "        W_out_grad += np.dot(z[:,t+1].reshape(-1,1), delta_out[:,t].reshape(-1,1))\n",
        "        W_grad += np.dot(z[:,t].reshape(-1,1), delta[:,t].reshape(1,-1))\n",
        "        W_in_grad += np.dot(X.T, delta[:,t].reshape(1,-1))\n",
        "    \n",
        "    # 勾配適用\n",
        "    W_in -= learning_rate * W_in_grad\n",
        "    W_out -= learning_rate * W_out_grad\n",
        "    W -= learning_rate * W_grad\n",
        "    \n",
        "    W_in_grad *= 0\n",
        "    W_out_grad *= 0\n",
        "    W_grad *= 0\n",
        "    \n",
        "\n",
        "    if(i % plot_interval == 0):\n",
        "        all_losses.append(all_loss)        \n",
        "        print(\"iters:\" + str(i))\n",
        "        print(\"Loss:\" + str(all_loss))\n",
        "        print(\"Pred:\" + str(out_bin))\n",
        "        print(\"True:\" + str(d_bin))\n",
        "        out_int = 0\n",
        "        for index,x in enumerate(reversed(out_bin)):\n",
        "            out_int += x * pow(2, index)\n",
        "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out_int))\n",
        "        print(\"------------\")\n",
        "\n",
        "lists = range(0, iters_num, plot_interval)\n",
        "plt.plot(lists, all_losses, label=\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iters:0\n",
            "Loss:0.9374614273671049\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 0 0 0 0]\n",
            "125 + 19 = 0\n",
            "------------\n",
            "iters:100\n",
            "Loss:0.9927400485092139\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[0 0 1 0 1 0 1 1]\n",
            "13 + 30 = 255\n",
            "------------\n",
            "iters:200\n",
            "Loss:0.9090746406898711\n",
            "Pred:[0 0 0 1 1 0 1 0]\n",
            "True:[0 0 0 1 1 0 0 0]\n",
            "9 + 15 = 26\n",
            "------------\n",
            "iters:300\n",
            "Loss:0.9793703920404274\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 0 0 1 1]\n",
            "80 + 3 = 0\n",
            "------------\n",
            "iters:400\n",
            "Loss:0.9513278915082003\n",
            "Pred:[0 0 0 0 1 0 1 1]\n",
            "True:[0 0 1 1 0 0 0 0]\n",
            "33 + 15 = 11\n",
            "------------\n",
            "iters:500\n",
            "Loss:1.0601545310908382\n",
            "Pred:[1 0 0 1 1 0 1 1]\n",
            "True:[0 1 1 0 0 0 1 0]\n",
            "29 + 69 = 155\n",
            "------------\n",
            "iters:600\n",
            "Loss:1.2985769681749415\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[1 0 0 0 0 1 0 1]\n",
            "17 + 116 = 255\n",
            "------------\n",
            "iters:700\n",
            "Loss:1.0115713186664195\n",
            "Pred:[0 1 0 1 1 0 0 0]\n",
            "True:[1 0 1 0 0 1 0 0]\n",
            "102 + 62 = 88\n",
            "------------\n",
            "iters:800\n",
            "Loss:1.0299672006211065\n",
            "Pred:[0 1 1 1 1 1 0 1]\n",
            "True:[0 1 0 0 1 1 1 0]\n",
            "64 + 14 = 125\n",
            "------------\n",
            "iters:900\n",
            "Loss:1.0043390314825795\n",
            "Pred:[0 0 0 0 1 0 1 0]\n",
            "True:[1 0 1 0 1 1 0 0]\n",
            "55 + 117 = 10\n",
            "------------\n",
            "iters:1000\n",
            "Loss:1.0036972115506237\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[1 0 0 0 1 1 1 1]\n",
            "92 + 51 = 255\n",
            "------------\n",
            "iters:1100\n",
            "Loss:0.6854806833125622\n",
            "Pred:[0 1 1 1 1 1 1 1]\n",
            "True:[0 0 1 0 1 1 1 1]\n",
            "17 + 30 = 127\n",
            "------------\n",
            "iters:1200\n",
            "Loss:0.950788936588495\n",
            "Pred:[0 0 0 0 0 1 0 0]\n",
            "True:[1 0 1 0 1 0 0 0]\n",
            "102 + 66 = 4\n",
            "------------\n",
            "iters:1300\n",
            "Loss:1.0266581238471133\n",
            "Pred:[1 0 0 0 0 0 1 0]\n",
            "True:[1 1 0 1 0 1 0 1]\n",
            "116 + 97 = 130\n",
            "------------\n",
            "iters:1400\n",
            "Loss:0.8397290865851892\n",
            "Pred:[1 0 1 0 0 0 0 1]\n",
            "True:[1 0 1 1 0 1 0 1]\n",
            "89 + 92 = 161\n",
            "------------\n",
            "iters:1500\n",
            "Loss:0.7015560688743712\n",
            "Pred:[0 0 0 1 0 1 0 1]\n",
            "True:[0 0 1 0 0 1 0 1]\n",
            "26 + 11 = 21\n",
            "------------\n",
            "iters:1600\n",
            "Loss:1.023127162777473\n",
            "Pred:[0 1 0 1 1 1 1 1]\n",
            "True:[0 1 1 0 0 0 1 1]\n",
            "93 + 6 = 95\n",
            "------------\n",
            "iters:1700\n",
            "Loss:0.6486369293312233\n",
            "Pred:[0 0 0 1 1 0 1 0]\n",
            "True:[0 0 1 1 1 0 1 0]\n",
            "53 + 5 = 26\n",
            "------------\n",
            "iters:1800\n",
            "Loss:0.8046196467858243\n",
            "Pred:[1 0 0 1 1 1 1 1]\n",
            "True:[1 1 0 1 1 1 0 1]\n",
            "112 + 109 = 159\n",
            "------------\n",
            "iters:1900\n",
            "Loss:1.0497222651751128\n",
            "Pred:[0 0 1 0 1 1 1 0]\n",
            "True:[0 0 1 1 0 0 0 0]\n",
            "13 + 35 = 46\n",
            "------------\n",
            "iters:2000\n",
            "Loss:0.786623717128748\n",
            "Pred:[0 1 1 0 1 0 0 1]\n",
            "True:[0 1 1 0 1 0 1 1]\n",
            "87 + 20 = 105\n",
            "------------\n",
            "iters:2100\n",
            "Loss:0.9273505246189133\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 0 1 1 0]\n",
            "70 + 80 = 0\n",
            "------------\n",
            "iters:2200\n",
            "Loss:0.632806085712423\n",
            "Pred:[0 0 1 0 1 0 1 0]\n",
            "True:[0 0 1 0 1 1 0 0]\n",
            "25 + 19 = 42\n",
            "------------\n",
            "iters:2300\n",
            "Loss:0.9555735713460001\n",
            "Pred:[0 1 0 1 1 1 0 0]\n",
            "True:[0 1 1 0 0 0 0 0]\n",
            "66 + 30 = 92\n",
            "------------\n",
            "iters:2400\n",
            "Loss:0.7098415895065063\n",
            "Pred:[0 0 1 1 0 1 0 1]\n",
            "True:[0 1 1 1 0 1 0 1]\n",
            "43 + 74 = 53\n",
            "------------\n",
            "iters:2500\n",
            "Loss:0.671100276582988\n",
            "Pred:[1 0 0 0 1 1 0 1]\n",
            "True:[1 0 1 0 1 1 0 1]\n",
            "47 + 126 = 141\n",
            "------------\n",
            "iters:2600\n",
            "Loss:0.9945485434473775\n",
            "Pred:[0 1 1 0 1 1 1 1]\n",
            "True:[1 0 0 0 0 1 1 1]\n",
            "20 + 115 = 111\n",
            "------------\n",
            "iters:2700\n",
            "Loss:0.8252853184066334\n",
            "Pred:[1 1 1 0 0 1 0 1]\n",
            "True:[1 1 1 1 0 0 1 1]\n",
            "119 + 124 = 229\n",
            "------------\n",
            "iters:2800\n",
            "Loss:0.5990558343692565\n",
            "Pred:[1 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 0 1 0 0]\n",
            "53 + 95 = 128\n",
            "------------\n",
            "iters:2900\n",
            "Loss:0.4413439827157665\n",
            "Pred:[1 0 0 0 1 1 1 0]\n",
            "True:[1 0 0 0 1 1 1 0]\n",
            "62 + 80 = 142\n",
            "------------\n",
            "iters:3000\n",
            "Loss:0.15385719915825613\n",
            "Pred:[0 1 1 0 1 0 0 0]\n",
            "True:[0 1 1 0 1 0 0 0]\n",
            "88 + 16 = 104\n",
            "------------\n",
            "iters:3100\n",
            "Loss:0.23634246191406927\n",
            "Pred:[0 1 1 0 1 0 0 0]\n",
            "True:[0 1 1 0 1 0 0 0]\n",
            "33 + 71 = 104\n",
            "------------\n",
            "iters:3200\n",
            "Loss:0.2647413532193373\n",
            "Pred:[0 1 0 1 1 1 1 0]\n",
            "True:[0 1 0 1 1 1 1 0]\n",
            "32 + 62 = 94\n",
            "------------\n",
            "iters:3300\n",
            "Loss:0.17759875369282924\n",
            "Pred:[0 1 0 0 0 1 0 1]\n",
            "True:[0 1 0 0 0 1 0 1]\n",
            "8 + 61 = 69\n",
            "------------\n",
            "iters:3400\n",
            "Loss:0.24840184540413746\n",
            "Pred:[1 0 0 1 0 1 0 0]\n",
            "True:[1 0 0 1 0 1 0 0]\n",
            "113 + 35 = 148\n",
            "------------\n",
            "iters:3500\n",
            "Loss:0.163861329097158\n",
            "Pred:[1 0 1 1 1 1 1 0]\n",
            "True:[1 0 1 1 1 1 1 0]\n",
            "119 + 71 = 190\n",
            "------------\n",
            "iters:3600\n",
            "Loss:0.05964222402885629\n",
            "Pred:[1 0 1 0 0 1 1 1]\n",
            "True:[1 0 1 0 0 1 1 1]\n",
            "72 + 95 = 167\n",
            "------------\n",
            "iters:3700\n",
            "Loss:0.05678373797646637\n",
            "Pred:[0 0 1 1 1 0 0 1]\n",
            "True:[0 0 1 1 1 0 0 1]\n",
            "9 + 48 = 57\n",
            "------------\n",
            "iters:3800\n",
            "Loss:0.03201048329823544\n",
            "Pred:[1 0 1 1 1 0 1 0]\n",
            "True:[1 0 1 1 1 0 1 0]\n",
            "113 + 73 = 186\n",
            "------------\n",
            "iters:3900\n",
            "Loss:0.04999352835309772\n",
            "Pred:[1 0 0 1 0 1 1 1]\n",
            "True:[1 0 0 1 0 1 1 1]\n",
            "51 + 100 = 151\n",
            "------------\n",
            "iters:4000\n",
            "Loss:0.1432930671481295\n",
            "Pred:[1 1 1 0 1 0 0 1]\n",
            "True:[1 1 1 0 1 0 0 1]\n",
            "125 + 108 = 233\n",
            "------------\n",
            "iters:4100\n",
            "Loss:0.033108005682590654\n",
            "Pred:[1 0 0 1 0 1 1 0]\n",
            "True:[1 0 0 1 0 1 1 0]\n",
            "94 + 56 = 150\n",
            "------------\n",
            "iters:4200\n",
            "Loss:0.06112877597860715\n",
            "Pred:[0 1 1 1 0 1 1 1]\n",
            "True:[0 1 1 1 0 1 1 1]\n",
            "16 + 103 = 119\n",
            "------------\n",
            "iters:4300\n",
            "Loss:0.02712019951034468\n",
            "Pred:[0 1 0 0 0 1 1 1]\n",
            "True:[0 1 0 0 0 1 1 1]\n",
            "7 + 64 = 71\n",
            "------------\n",
            "iters:4400\n",
            "Loss:0.03688353607943866\n",
            "Pred:[0 0 1 0 0 0 0 1]\n",
            "True:[0 0 1 0 0 0 0 1]\n",
            "24 + 9 = 33\n",
            "------------\n",
            "iters:4500\n",
            "Loss:0.032316897081429485\n",
            "Pred:[0 0 1 0 0 1 1 0]\n",
            "True:[0 0 1 0 0 1 1 0]\n",
            "16 + 22 = 38\n",
            "------------\n",
            "iters:4600\n",
            "Loss:0.014806746204782022\n",
            "Pred:[1 1 0 0 1 0 0 1]\n",
            "True:[1 1 0 0 1 0 0 1]\n",
            "93 + 108 = 201\n",
            "------------\n",
            "iters:4700\n",
            "Loss:0.016798910462824463\n",
            "Pred:[0 1 0 1 0 1 0 0]\n",
            "True:[0 1 0 1 0 1 0 0]\n",
            "57 + 27 = 84\n",
            "------------\n",
            "iters:4800\n",
            "Loss:0.02659898494672353\n",
            "Pred:[1 1 0 1 1 0 0 0]\n",
            "True:[1 1 0 1 1 0 0 0]\n",
            "90 + 126 = 216\n",
            "------------\n",
            "iters:4900\n",
            "Loss:0.020674424348632873\n",
            "Pred:[0 0 1 1 0 1 1 0]\n",
            "True:[0 0 1 1 0 1 1 0]\n",
            "48 + 6 = 54\n",
            "------------\n",
            "iters:5000\n",
            "Loss:0.020640338752982653\n",
            "Pred:[1 0 0 0 0 0 1 0]\n",
            "True:[1 0 0 0 0 0 1 0]\n",
            "94 + 36 = 130\n",
            "------------\n",
            "iters:5100\n",
            "Loss:0.019099571046355682\n",
            "Pred:[0 1 1 1 0 1 0 0]\n",
            "True:[0 1 1 1 0 1 0 0]\n",
            "76 + 40 = 116\n",
            "------------\n",
            "iters:5200\n",
            "Loss:0.021968902115029085\n",
            "Pred:[1 1 0 0 1 0 0 0]\n",
            "True:[1 1 0 0 1 0 0 0]\n",
            "113 + 87 = 200\n",
            "------------\n",
            "iters:5300\n",
            "Loss:0.019318937150117566\n",
            "Pred:[1 0 1 0 1 1 1 0]\n",
            "True:[1 0 1 0 1 1 1 0]\n",
            "87 + 87 = 174\n",
            "------------\n",
            "iters:5400\n",
            "Loss:0.011094598533877004\n",
            "Pred:[0 1 1 0 0 1 0 1]\n",
            "True:[0 1 1 0 0 1 0 1]\n",
            "11 + 90 = 101\n",
            "------------\n",
            "iters:5500\n",
            "Loss:0.01571825953186665\n",
            "Pred:[0 0 1 1 1 0 1 0]\n",
            "True:[0 0 1 1 1 0 1 0]\n",
            "52 + 6 = 58\n",
            "------------\n",
            "iters:5600\n",
            "Loss:0.00587141061825357\n",
            "Pred:[1 1 0 1 1 0 0 1]\n",
            "True:[1 1 0 1 1 0 0 1]\n",
            "121 + 96 = 217\n",
            "------------\n",
            "iters:5700\n",
            "Loss:0.01039531783690938\n",
            "Pred:[1 1 0 1 0 0 0 1]\n",
            "True:[1 1 0 1 0 0 0 1]\n",
            "124 + 85 = 209\n",
            "------------\n",
            "iters:5800\n",
            "Loss:0.007696229075262573\n",
            "Pred:[1 0 1 0 1 1 1 1]\n",
            "True:[1 0 1 0 1 1 1 1]\n",
            "117 + 58 = 175\n",
            "------------\n",
            "iters:5900\n",
            "Loss:0.014869901616447788\n",
            "Pred:[0 0 0 1 1 0 1 0]\n",
            "True:[0 0 0 1 1 0 1 0]\n",
            "12 + 14 = 26\n",
            "------------\n",
            "iters:6000\n",
            "Loss:0.00887032318833588\n",
            "Pred:[0 1 1 0 1 1 0 1]\n",
            "True:[0 1 1 0 1 1 0 1]\n",
            "80 + 29 = 109\n",
            "------------\n",
            "iters:6100\n",
            "Loss:0.004828016788522145\n",
            "Pred:[1 0 1 0 1 1 0 0]\n",
            "True:[1 0 1 0 1 1 0 0]\n",
            "89 + 83 = 172\n",
            "------------\n",
            "iters:6200\n",
            "Loss:0.011704094711432897\n",
            "Pred:[1 0 0 1 1 0 1 0]\n",
            "True:[1 0 0 1 1 0 1 0]\n",
            "50 + 104 = 154\n",
            "------------\n",
            "iters:6300\n",
            "Loss:0.006133838447110499\n",
            "Pred:[1 1 0 1 0 0 0 1]\n",
            "True:[1 1 0 1 0 0 0 1]\n",
            "82 + 127 = 209\n",
            "------------\n",
            "iters:6400\n",
            "Loss:0.006625816433744957\n",
            "Pred:[0 1 1 1 0 0 1 1]\n",
            "True:[0 1 1 1 0 0 1 1]\n",
            "82 + 33 = 115\n",
            "------------\n",
            "iters:6500\n",
            "Loss:0.005532170729261007\n",
            "Pred:[1 0 0 1 1 0 1 1]\n",
            "True:[1 0 0 1 1 0 1 1]\n",
            "69 + 86 = 155\n",
            "------------\n",
            "iters:6600\n",
            "Loss:0.00682293020191016\n",
            "Pred:[0 1 0 0 1 0 0 0]\n",
            "True:[0 1 0 0 1 0 0 0]\n",
            "4 + 68 = 72\n",
            "------------\n",
            "iters:6700\n",
            "Loss:0.0030997187297049744\n",
            "Pred:[0 1 0 1 0 0 0 0]\n",
            "True:[0 1 0 1 0 0 0 0]\n",
            "39 + 41 = 80\n",
            "------------\n",
            "iters:6800\n",
            "Loss:0.003140914692911143\n",
            "Pred:[0 1 1 0 1 0 1 1]\n",
            "True:[0 1 1 0 1 0 1 1]\n",
            "83 + 24 = 107\n",
            "------------\n",
            "iters:6900\n",
            "Loss:0.00993807490062713\n",
            "Pred:[1 1 0 1 0 0 1 0]\n",
            "True:[1 1 0 1 0 0 1 0]\n",
            "108 + 102 = 210\n",
            "------------\n",
            "iters:7000\n",
            "Loss:0.00715991434529414\n",
            "Pred:[1 0 0 0 1 1 1 0]\n",
            "True:[1 0 0 0 1 1 1 0]\n",
            "84 + 58 = 142\n",
            "------------\n",
            "iters:7100\n",
            "Loss:0.00417063715061938\n",
            "Pred:[1 0 0 0 1 1 1 1]\n",
            "True:[1 0 0 0 1 1 1 1]\n",
            "117 + 26 = 143\n",
            "------------\n",
            "iters:7200\n",
            "Loss:0.0018953457319099037\n",
            "Pred:[1 1 1 0 0 1 1 0]\n",
            "True:[1 1 1 0 0 1 1 0]\n",
            "105 + 125 = 230\n",
            "------------\n",
            "iters:7300\n",
            "Loss:0.006453975123937355\n",
            "Pred:[1 1 1 1 0 0 0 0]\n",
            "True:[1 1 1 1 0 0 0 0]\n",
            "124 + 116 = 240\n",
            "------------\n",
            "iters:7400\n",
            "Loss:0.004287100785316417\n",
            "Pred:[1 1 0 1 1 1 1 1]\n",
            "True:[1 1 0 1 1 1 1 1]\n",
            "122 + 101 = 223\n",
            "------------\n",
            "iters:7500\n",
            "Loss:0.005579978181692682\n",
            "Pred:[1 0 1 1 1 0 0 0]\n",
            "True:[1 0 1 1 1 0 0 0]\n",
            "62 + 122 = 184\n",
            "------------\n",
            "iters:7600\n",
            "Loss:0.002788303795864223\n",
            "Pred:[1 0 1 1 1 1 0 1]\n",
            "True:[1 0 1 1 1 1 0 1]\n",
            "81 + 108 = 189\n",
            "------------\n",
            "iters:7700\n",
            "Loss:0.0036008830114751564\n",
            "Pred:[1 0 0 0 1 1 1 0]\n",
            "True:[1 0 0 0 1 1 1 0]\n",
            "103 + 39 = 142\n",
            "------------\n",
            "iters:7800\n",
            "Loss:0.004487751409907132\n",
            "Pred:[1 0 1 1 1 0 1 1]\n",
            "True:[1 0 1 1 1 0 1 1]\n",
            "120 + 67 = 187\n",
            "------------\n",
            "iters:7900\n",
            "Loss:0.00344722780706932\n",
            "Pred:[0 1 1 1 1 0 1 1]\n",
            "True:[0 1 1 1 1 0 1 1]\n",
            "13 + 110 = 123\n",
            "------------\n",
            "iters:8000\n",
            "Loss:0.003047595529874686\n",
            "Pred:[1 0 0 1 1 0 1 1]\n",
            "True:[1 0 0 1 1 0 1 1]\n",
            "81 + 74 = 155\n",
            "------------\n",
            "iters:8100\n",
            "Loss:0.0029850813234901237\n",
            "Pred:[0 0 1 0 0 1 0 1]\n",
            "True:[0 0 1 0 0 1 0 1]\n",
            "3 + 34 = 37\n",
            "------------\n",
            "iters:8200\n",
            "Loss:0.004476737764205197\n",
            "Pred:[1 0 1 0 0 0 1 0]\n",
            "True:[1 0 1 0 0 0 1 0]\n",
            "72 + 90 = 162\n",
            "------------\n",
            "iters:8300\n",
            "Loss:0.003154556964370586\n",
            "Pred:[0 1 1 0 0 1 1 1]\n",
            "True:[0 1 1 0 0 1 1 1]\n",
            "78 + 25 = 103\n",
            "------------\n",
            "iters:8400\n",
            "Loss:0.0032501802045444097\n",
            "Pred:[0 1 1 1 1 0 1 1]\n",
            "True:[0 1 1 1 1 0 1 1]\n",
            "114 + 9 = 123\n",
            "------------\n",
            "iters:8500\n",
            "Loss:0.0018833257655334027\n",
            "Pred:[0 1 1 0 1 1 0 1]\n",
            "True:[0 1 1 0 1 1 0 1]\n",
            "101 + 8 = 109\n",
            "------------\n",
            "iters:8600\n",
            "Loss:0.002696303896026326\n",
            "Pred:[1 0 0 1 0 1 1 0]\n",
            "True:[1 0 0 1 0 1 1 0]\n",
            "95 + 55 = 150\n",
            "------------\n",
            "iters:8700\n",
            "Loss:0.0040736378586694695\n",
            "Pred:[0 1 0 0 0 0 1 0]\n",
            "True:[0 1 0 0 0 0 1 0]\n",
            "8 + 58 = 66\n",
            "------------\n",
            "iters:8800\n",
            "Loss:0.002101700098814717\n",
            "Pred:[0 1 1 1 0 0 0 1]\n",
            "True:[0 1 1 1 0 0 0 1]\n",
            "3 + 110 = 113\n",
            "------------\n",
            "iters:8900\n",
            "Loss:0.003555518243795775\n",
            "Pred:[0 0 1 0 1 1 0 0]\n",
            "True:[0 0 1 0 1 1 0 0]\n",
            "40 + 4 = 44\n",
            "------------\n",
            "iters:9000\n",
            "Loss:0.0022637147738735425\n",
            "Pred:[1 1 0 0 0 1 1 0]\n",
            "True:[1 1 0 0 0 1 1 0]\n",
            "87 + 111 = 198\n",
            "------------\n",
            "iters:9100\n",
            "Loss:0.002933513250899854\n",
            "Pred:[1 0 0 0 1 1 1 0]\n",
            "True:[1 0 0 0 1 1 1 0]\n",
            "114 + 28 = 142\n",
            "------------\n",
            "iters:9200\n",
            "Loss:0.0018722171415554636\n",
            "Pred:[1 1 1 1 1 0 1 0]\n",
            "True:[1 1 1 1 1 0 1 0]\n",
            "127 + 123 = 250\n",
            "------------\n",
            "iters:9300\n",
            "Loss:0.002601248615658758\n",
            "Pred:[1 0 1 0 1 1 1 1]\n",
            "True:[1 0 1 0 1 1 1 1]\n",
            "90 + 85 = 175\n",
            "------------\n",
            "iters:9400\n",
            "Loss:0.0006677844203735112\n",
            "Pred:[1 1 0 1 0 0 1 0]\n",
            "True:[1 1 0 1 0 0 1 0]\n",
            "97 + 113 = 210\n",
            "------------\n",
            "iters:9500\n",
            "Loss:0.0005726032013142277\n",
            "Pred:[0 0 0 0 0 1 0 0]\n",
            "True:[0 0 0 0 0 1 0 0]\n",
            "3 + 1 = 4\n",
            "------------\n",
            "iters:9600\n",
            "Loss:0.0027025605986606637\n",
            "Pred:[0 1 0 1 1 0 0 0]\n",
            "True:[0 1 0 1 1 0 0 0]\n",
            "42 + 46 = 88\n",
            "------------\n",
            "iters:9700\n",
            "Loss:0.0017925889838014918\n",
            "Pred:[0 1 1 0 1 0 0 1]\n",
            "True:[0 1 1 0 1 0 0 1]\n",
            "59 + 46 = 105\n",
            "------------\n",
            "iters:9800\n",
            "Loss:0.001780669723072224\n",
            "Pred:[0 0 1 1 0 0 1 1]\n",
            "True:[0 0 1 1 0 0 1 1]\n",
            "48 + 3 = 51\n",
            "------------\n",
            "iters:9900\n",
            "Loss:0.0023285951269519374\n",
            "Pred:[0 1 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 0 0 0 0]\n",
            "62 + 2 = 64\n",
            "------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsBt7vxmO-bn"
      },
      "source": [
        "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "## [try] weight_init_stdやlearning_rate, hidden_layer_sizeを変更してみよう\n",
        "\n",
        "\n",
        "## [try] 重みの初期化方法を変更してみよう\n",
        "Xavier, He\n",
        "\n",
        "## [try] 中間層の活性化関数を変更してみよう\n",
        "ReLU(勾配爆発を確認しよう)<br>\n",
        "tanh(numpyにtanhが用意されている。導関数をd_tanhとして作成しよう)\n",
        "\n",
        "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    }
  ]
}